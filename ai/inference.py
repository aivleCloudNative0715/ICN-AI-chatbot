import torch
import torch.nn as nn
from torch.nn.functional import softmax
from transformers import BertModel, BertTokenizer
import pickle

# ğŸ§  ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜
class KoBERTClassifier(nn.Module):
    def __init__(self, num_labels):
        super(KoBERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained("monologg/kobert")
        self.dropout = nn.Dropout(0.3)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        return self.classifier(self.dropout(pooled_output))

# ğŸ”§ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… í† í¬ë‚˜ì´ì € ë° ë¼ë²¨ ì¸ì½”ë” ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained("monologg/kobert")
with open("label_encoder.pkl", "rb") as f:
    label_encoder = pickle.load(f)

# âœ… ëª¨ë¸ ë¡œë“œ
model = KoBERTClassifier(num_labels=len(label_encoder.classes_))
model.load_state_dict(torch.load("best_kobert_model.pt", map_location=device))
model.to(device)
model.eval()

# ğŸ”® ì˜ˆì¸¡ í•¨ìˆ˜
def predict_intent(text):
    encoding = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=64,
        return_tensors='pt'
    )
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask)
        probs = softmax(outputs, dim=1)
        conf, predicted = torch.max(probs, 1)

    intent = label_encoder.inverse_transform([predicted.item()])[0]
    return intent, conf.item()

# ğŸ” í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    while True:
        text = input("âœ‰ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
        if text.lower() == 'exit':
            break
        intent, confidence = predict_intent(text)
        print(f"ğŸ”– ì˜ˆì¸¡ ì¸í…íŠ¸: {intent} | ğŸ” Confidence: {confidence:.4f}")
