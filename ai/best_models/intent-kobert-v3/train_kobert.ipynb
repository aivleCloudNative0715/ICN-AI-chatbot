{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install konlpy",
   "id": "5f8eac3e73f02181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class KoBERTIntentSlotModel(nn.Module):\n",
    "    def __init__(self, num_intents, num_slots):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.intent_classifier = nn.Linear(hidden_size, num_intents)\n",
    "        self.slot_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_slots)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        return intent_logits, slot_logits\n"
   ],
   "id": "f28682c2045059b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "def load_and_preprocess_data():\n",
    "    df = pd.read_csv(\"intent_slot_dataset_cleaned.csv\")\n",
    "\n",
    "    df[\"intent_list\"] = df[\"intent_list\"].apply(\n",
    "        lambda x: json.loads(x) if isinstance(x, str) and x.strip().startswith(\"[\") else (x if isinstance(x, list) else [])\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_to_bio_word_level(sentence, slot_json):\n",
    "    tokens = sentence.split()\n",
    "    labels = ['O'] * len(tokens)\n",
    "\n",
    "    for slot_name, slot_value in slot_json.items():\n",
    "        slot_values = slot_value if isinstance(slot_value, list) else [slot_value]\n",
    "        for val in slot_values:\n",
    "            if not isinstance(val, str):\n",
    "                continue\n",
    "            val_tokens = val.split()\n",
    "\n",
    "            # âœ… ìˆ˜ì •: ëª¨ë“  ë§¤ì¹­ì„ ì°¾ì•„ì„œ ì²˜ë¦¬\n",
    "            i = 0\n",
    "            while i <= len(tokens) - len(val_tokens):\n",
    "                if tokens[i:i+len(val_tokens)] == val_tokens:\n",
    "                    # ì´ë¯¸ íƒœê¹…ëœ ë¶€ë¶„ì´ ì•„ë‹ ë•Œë§Œ íƒœê¹…\n",
    "                    if all(labels[i+k] == 'O' for k in range(len(val_tokens))):\n",
    "                        labels[i] = f'B-{slot_name}'\n",
    "                        for j in range(1, len(val_tokens)):\n",
    "                            labels[i + j] = f'I-{slot_name}'\n",
    "                    i += len(val_tokens)  # ë§¤ì¹­ëœ ë¶€ë¶„ ë‹¤ìŒë¶€í„° ê³„ì† ì°¾ê¸°\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "    return tokens, labels\n",
    "\n",
    "def create_bio_dataset(df_combined):\n",
    "    bio_data = []\n",
    "    for idx, (_, row) in enumerate(df_combined.iterrows()):\n",
    "        sentence = str(row['question']).strip()\n",
    "        if not sentence or sentence.lower() == 'nan':\n",
    "            continue\n",
    "\n",
    "        raw_slots = row['slots']\n",
    "        try:\n",
    "            if isinstance(raw_slots, str):\n",
    "                if raw_slots.strip() in ['', '[]', '{}', 'nan']:\n",
    "                    slot_dict = {}\n",
    "                else:\n",
    "                    cleaned = raw_slots.replace('null','None').replace('true','True').replace('false','False')\n",
    "                    slot_dict = ast.literal_eval(cleaned)\n",
    "            elif isinstance(raw_slots, dict):\n",
    "                slot_dict = raw_slots\n",
    "            else:\n",
    "                slot_dict = {}\n",
    "\n",
    "            tokens, labels = convert_to_bio_word_level(sentence, slot_dict)\n",
    "            bio_data.append({\n",
    "                \"intent_list\": row.get(\"intent_list\", []),  # âœ… ì—¬ê¸°ë§Œ ìœ ì§€\n",
    "                \"tokens\": tokens,\n",
    "                \"labels\": labels\n",
    "            })\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame(bio_data)\n",
    "\n",
    "def create_mappings(df_bio):\n",
    "      \"\"\"Intent ë° Slot ë§¤í•‘ ìƒì„± - Multi-label ì§€ì›\"\"\"\n",
    "      df_bio['labels'] = df_bio['labels'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "      # Multi-label intent ì²˜ë¦¬: ëª¨ë“  ê°œë³„ intentë¥¼ ìˆ˜ì§‘\n",
    "      all_intents = set()\n",
    "      for intent_list in df_bio['intent_list']:\n",
    "          if isinstance(intent_list, list):\n",
    "              all_intents.update(intent_list)\n",
    "          else:\n",
    "              all_intents.add(intent_list)\n",
    "\n",
    "      # Intent ë§¤í•‘ (ê°œë³„ intent ê¸°ì¤€)\n",
    "      intent_labels = sorted(all_intents)\n",
    "      intent2idx = {label: i for i, label in enumerate(intent_labels)}\n",
    "      idx2intent = {i: label for label, i in intent2idx.items()}\n",
    "\n",
    "      # Slot ë¼ë²¨ ìˆ˜ì§‘ ë° B/I í™•ì¥\n",
    "      original_labels = set()\n",
    "      for label_list in df_bio['labels']:\n",
    "          for label in label_list:\n",
    "              original_labels.add(label)\n",
    "              if label.startswith('B-'):\n",
    "                  original_labels.add('I-' + label[2:])\n",
    "\n",
    "      # Slot ë§¤í•‘\n",
    "      slot_labels = sorted(original_labels)\n",
    "      slot2idx = {label: i for i, label in enumerate(slot_labels)}\n",
    "      idx2slot = {i: label for label, i in slot2idx.items()}\n",
    "\n",
    "      return intent2idx, idx2intent, slot2idx, idx2slot\n",
    "\n",
    "def create_balanced_dataset(df_bio, target_intent_samples=1155, target_slot_samples=300):\n",
    "    \"\"\"ë°ì´í„° ë°¸ëŸ°ì‹± - Multi-label (intent_list) ì§€ì›\"\"\"\n",
    "\n",
    "    # âœ… stratify/ê·¸ë£¹í•‘ìš© ì„ì‹œ í‚¤: intent_listë¥¼ ì •ë ¬ íŠœí”Œë¡œ\n",
    "    intent_tuple_key = df_bio['intent_list'].apply(lambda x: tuple(sorted(x)) if isinstance(x, list) else (str(x),))\n",
    "\n",
    "    # â¬‡ï¸ intent ì¡°í•©ë³„ë¡œ ì—…/ë‹¤ìš´ìƒ˜í”Œ\n",
    "    balanced_intent_dfs = []\n",
    "    for combo in intent_tuple_key.unique():\n",
    "        mask = intent_tuple_key == combo\n",
    "        intent_df = df_bio[mask]\n",
    "\n",
    "        if len(intent_df) >= target_intent_samples:\n",
    "            sampled_df = resample(intent_df, replace=False, n_samples=target_intent_samples, random_state=42)\n",
    "        else:\n",
    "            sampled_df = resample(intent_df, replace=True,  n_samples=target_intent_samples, random_state=42)\n",
    "\n",
    "        balanced_intent_dfs.append(sampled_df)\n",
    "\n",
    "    df_balanced_intent = pd.concat(balanced_intent_dfs, ignore_index=True)\n",
    "\n",
    "    # â¬‡ï¸ ìŠ¬ë¡¯ ë¼ë²¨ ë¶„í¬ ë³´ì •\n",
    "    slot_counter = Counter(label for labels in df_balanced_intent['labels'] for label in labels)\n",
    "    rare_slots = [label for label, cnt in slot_counter.items() if cnt < target_slot_samples]\n",
    "\n",
    "    slot_augmented_dfs = []\n",
    "    for rare_label in rare_slots:\n",
    "        slot_df = df_bio[df_bio['labels'].apply(lambda lst: rare_label in lst)]\n",
    "        if len(slot_df) == 0:\n",
    "            continue\n",
    "        needed = target_slot_samples - slot_counter[rare_label]\n",
    "        if needed > 0:\n",
    "            dup_df = resample(slot_df, replace=True, n_samples=needed, random_state=42)\n",
    "            slot_augmented_dfs.append(dup_df)\n",
    "\n",
    "    df_balanced = pd.concat([df_balanced_intent] + slot_augmented_dfs, ignore_index=True) if slot_augmented_dfs else df_balanced_intent\n",
    "\n",
    "    # âœ… ë””ë²„ê·¸ìš© ì¶œë ¥(ì˜ë„ ì¡°í•© ë¶„í¬)\n",
    "    counts = intent_tuple_key.value_counts()\n",
    "    print(\"âœ… Intent ì¡°í•© ê°œìˆ˜:\", len(counts))\n",
    "    print(\"ìƒìœ„ 10ê°œ ì¡°í•©:\")\n",
    "    for combo, cnt in counts.head(10).items():\n",
    "        print(f\"  {combo}: {cnt}\")\n",
    "\n",
    "    print(f\"\\nâœ… ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {len(df_balanced)}\")\n",
    "    return df_balanced\n",
    "\n",
    "# ğŸ·ï¸ BCEWithLogitsLoss ì§€ì› Dataset í´ë˜ìŠ¤\n",
    "class IntentSlotDataset(Dataset):\n",
    "    def __init__(self, encodings, slot_labels, intents, intent2idx, use_bce=True):\n",
    "        self.encodings = encodings\n",
    "        self.slot_labels = slot_labels\n",
    "        self.intents = intents\n",
    "        self.intent2idx = intent2idx\n",
    "        self.use_bce = use_bce\n",
    "        self.num_intents = len(intent2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.intents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.slot_labels[idx])\n",
    "\n",
    "        # Multi-label intentë¥¼ one-hot vectorë¡œ ë³€í™˜\n",
    "        intent_vector = torch.zeros(self.num_intents)\n",
    "        intent_labels = self.intents[idx]\n",
    "\n",
    "        if isinstance(intent_labels, list):\n",
    "            for intent_idx in intent_labels:\n",
    "                intent_vector[intent_idx] = 1\n",
    "        else:\n",
    "            intent_vector[intent_labels] = 1\n",
    "\n",
    "        item['intent'] = intent_vector\n",
    "        return item\n",
    "\n",
    "def align_labels_with_tokenizer(tokens, labels, tokenizer):\n",
    "    \"\"\"í† í°ê³¼ ë¼ë²¨ ì •ë ¬\"\"\"\n",
    "    bert_tokens = []\n",
    "    aligned_labels = []\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        if not sub_tokens:\n",
    "            sub_tokens = [tokenizer.unk_token]\n",
    "\n",
    "        bert_tokens.extend(sub_tokens)\n",
    "        aligned_labels.append(label)\n",
    "\n",
    "        for _ in range(1, len(sub_tokens)):\n",
    "            if label == \"O\":\n",
    "                aligned_labels.append(\"O\")\n",
    "            elif label.startswith(\"B-\"):\n",
    "                aligned_labels.append(\"I-\" + label[2:])\n",
    "            elif label.startswith(\"I-\"):\n",
    "                aligned_labels.append(label)\n",
    "            else:\n",
    "                aligned_labels.append(\"O\")\n",
    "\n",
    "    return bert_tokens, aligned_labels\n",
    "\n",
    "def _ensure_list(x):\n",
    "    # intent_listê°€ ë¦¬ìŠ¤íŠ¸/JSON/ë¬¸ìì—´ ì–´ë–¤ í˜•íƒœì—¬ë„ ë¦¬ìŠ¤íŠ¸ë¡œ í‘œì¤€í™”\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s.startswith('[') and s.endswith(']'):\n",
    "            try:\n",
    "                arr = json.loads(s)\n",
    "                return [str(v).strip() for v in arr if str(v).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        return [t.strip() for t in s.split(',') if t.strip()]\n",
    "    return [] if pd.isna(x) else [str(x).strip()]\n",
    "\n",
    "def encode_data(df, tokenizer, intent2idx, slot2idx, max_len=64):\n",
    "    \"\"\"ë°ì´í„° ì¸ì½”ë”© - Multi-label (intent_list) ì§€ì›\"\"\"\n",
    "    input_ids, attention_masks, slot_label_ids, intent_ids = [], [], [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        tokens = row[\"tokens\"]\n",
    "        labels = row[\"labels\"]\n",
    "\n",
    "        # í† í¬ë‚˜ì´ì € ì •ë ¬\n",
    "        bert_tokens, aligned_labels = align_labels_with_tokenizer(tokens, labels, tokenizer)\n",
    "        label_ids = [slot2idx[label] for label in aligned_labels]\n",
    "\n",
    "        # ê¸¸ì´ ìë¥´ê¸°\n",
    "        if len(bert_tokens) > max_len - 2:\n",
    "            bert_tokens = bert_tokens[:max_len - 2]\n",
    "            label_ids   = label_ids[:max_len - 2]\n",
    "\n",
    "        # [CLS], [SEP] / íŒ¨ë”©\n",
    "        tokens_input = ['[CLS]'] + bert_tokens + ['[SEP]']\n",
    "        label_ids    = [slot2idx['O']] + label_ids + [slot2idx['O']]\n",
    "\n",
    "        input_id       = tokenizer.convert_tokens_to_ids(tokens_input)\n",
    "        attention_mask = [1] * len(input_id)\n",
    "\n",
    "        pad_len = max_len - len(input_id)\n",
    "        if pad_len > 0:\n",
    "            input_id       += [0] * pad_len\n",
    "            attention_mask += [0] * pad_len\n",
    "            label_ids      += [slot2idx['O']] * pad_len\n",
    "\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        slot_label_ids.append(label_ids)\n",
    "\n",
    "        # âœ… Multi-label intent_list â†’ index ë¦¬ìŠ¤íŠ¸\n",
    "        intents = _ensure_list(row.get(\"intent_list\", []))\n",
    "        intent_indices = [intent2idx[i] for i in intents if i in intent2idx]\n",
    "\n",
    "        # ìµœì†Œ 1ê°œëŠ” ë³´ì¥(ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ìœ ì§€í•´ë„ ë¨; Datasetì—ì„œ ì›í•« ë§Œë“¤ ë•Œ 0ìœ¼ë¡œë§Œ ì±„ì›Œì§)\n",
    "        intent_ids.append(intent_indices)\n",
    "\n",
    "    encodings = {\"input_ids\": input_ids, \"attention_mask\": attention_masks}\n",
    "    return encodings, slot_label_ids, intent_ids\n",
    "\n",
    "def create_datasets(df_balanced, tokenizer, intent2idx, slot2idx, use_bce=True, max_len=64, test_size=0.1):\n",
    "      \"\"\"Dataset ë° DataLoader ìƒì„± - Stratify ë¬¸ì œ í•´ê²°\"\"\"\n",
    "\n",
    "      df_balanced['intent_tuple'] = df_balanced['intent_list'].apply(lambda x: tuple(sorted(x)))\n",
    "\n",
    "      # Train/Val ë¶„í• \n",
    "      train_df, val_df = train_test_split(\n",
    "          df_balanced,\n",
    "          test_size=test_size,\n",
    "          stratify=df_balanced['intent_tuple'],\n",
    "          random_state=42\n",
    "      )\n",
    "\n",
    "      # intent_str ì»¬ëŸ¼ ì œê±°\n",
    "      train_df = train_df.drop(columns=['intent_tuple'])\n",
    "      val_df = val_df.drop(columns=['intent_tuple'])\n",
    "\n",
    "      # ì¸ì½”ë”©\n",
    "      train_encodings, train_slot_labels, train_intents = encode_data(train_df, tokenizer, intent2idx, slot2idx,\n",
    "  max_len)\n",
    "      val_encodings, val_slot_labels, val_intents = encode_data(val_df, tokenizer, intent2idx, slot2idx, max_len)\n",
    "\n",
    "      # Dataset ìƒì„±\n",
    "      train_dataset = IntentSlotDataset(train_encodings, train_slot_labels, train_intents, intent2idx, use_bce)\n",
    "      val_dataset = IntentSlotDataset(val_encodings, val_slot_labels, val_intents, intent2idx, use_bce)\n",
    "\n",
    "      # DataLoader ìƒì„±\n",
    "      train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "      val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "      print(f\"âœ… {'BCE' if use_bce else 'CrossEntropy'}ìš© ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ\")\n",
    "      print(f\"   Train: {len(train_dataset)} samples\")\n",
    "      print(f\"   Val: {len(val_dataset)} samples\")\n",
    "\n",
    "      return train_loader, val_loader\n",
    "\n",
    "# ğŸ”§ í•™ìŠµ ë° í‰ê°€ í•¨ìˆ˜\n",
    "def train_epoch_bce(model, dataloader, optimizer, device, intent2idx, slot2idx, slot_weights=None):\n",
    "    \"\"\"BCE ì†ì‹¤ì„ ì‚¬ìš©í•œ í•™ìŠµ\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loss functions\n",
    "    intent_loss_fn = BCEWithLogitsLoss()\n",
    "    if slot_weights is not None:\n",
    "        slot_loss_fn = nn.CrossEntropyLoss(weight=slot_weights)\n",
    "    else:\n",
    "        slot_loss_fn = CrossEntropyLoss()\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"ğŸ› ï¸ Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        slot_labels = batch['labels'].to(device)\n",
    "        intent_labels = batch['intent'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        intent_logits, slot_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Intent loss (BCE)\n",
    "        loss_intent = intent_loss_fn(intent_logits, intent_labels)\n",
    "\n",
    "        # Slot loss (CrossEntropy)\n",
    "        loss_slot = slot_loss_fn(slot_logits.view(-1, len(slot2idx)), slot_labels.view(-1))\n",
    "\n",
    "        loss = loss_intent + loss_slot\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate_bce(model, dataloader, device, intent2idx, slot2idx, threshold=0.5):\n",
    "    \"\"\"BCE ê¸°ë°˜ í‰ê°€\"\"\"\n",
    "    model.eval()\n",
    "    intent_preds, intent_trues = [], []\n",
    "    slot_preds, slot_trues = [], []\n",
    "\n",
    "    idx2intent = {v: k for k, v in intent2idx.items()}\n",
    "    idx2slot = {v: k for k, v in slot2idx.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"ğŸ” Validating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            slot_labels = batch['labels'].to(device)\n",
    "            intent_labels = batch['intent'].to(device)\n",
    "\n",
    "            intent_logits, slot_logits = model(input_ids, attention_mask)\n",
    "\n",
    "            # Intent ì˜ˆì¸¡ (sigmoid + threshold)\n",
    "            intent_probs = torch.sigmoid(intent_logits)\n",
    "            intent_pred = (intent_probs > threshold).float()\n",
    "\n",
    "            intent_preds.extend(intent_pred.cpu().numpy())\n",
    "            intent_trues.extend(intent_labels.cpu().numpy())\n",
    "\n",
    "            # Slot ì˜ˆì¸¡\n",
    "            slot_pred = torch.argmax(slot_logits, dim=2)\n",
    "            for i in range(slot_labels.size(0)):\n",
    "                true_seq = slot_labels[i].cpu().tolist()\n",
    "                pred_seq = slot_pred[i].cpu().tolist()\n",
    "                for t, p in zip(true_seq, pred_seq):\n",
    "                    if t != -100:\n",
    "                        slot_trues.append(t)\n",
    "                        slot_preds.append(p)\n",
    "\n",
    "    # Intent ì •í™•ë„ ê³„ì‚°\n",
    "    intent_preds = np.array(intent_preds)\n",
    "    intent_trues = np.array(intent_trues)\n",
    "\n",
    "    exact_match = np.all(intent_preds == intent_trues, axis=1)\n",
    "    intent_acc = np.mean(exact_match)\n",
    "\n",
    "    intent_report = classification_report(\n",
    "        intent_trues, intent_preds,\n",
    "        target_names=list(intent2idx.keys()),\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    # Slot ì •í™•ë„ ê³„ì‚°\n",
    "    support_counter = Counter(slot_trues)\n",
    "    nonzero_labels = [i for i in slot2idx.values() if support_counter[i] > 0]\n",
    "    target_names_nonzero = [key for key, val in slot2idx.items() if val in nonzero_labels]\n",
    "\n",
    "    slot_acc = accuracy_score(slot_trues, slot_preds)\n",
    "    slot_report = classification_report(\n",
    "        slot_trues, slot_preds,\n",
    "        labels=nonzero_labels,\n",
    "        target_names=target_names_nonzero,\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    return intent_acc, intent_report, slot_acc, slot_report\n",
    "\n",
    "def train_with_bce(model, train_loader, val_loader, device, intent2idx, slot2idx,\n",
    "                   slot_weights=None, epochs=10, lr=5e-5, threshold=0.5, save_path=None):\n",
    "    \"\"\"BCEWithLogitsLossë¡œ í•™ìŠµ\"\"\"\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥ìš©\n",
    "    train_losses = []\n",
    "    val_intent_accuracies = []\n",
    "    val_slot_accuracies = []\n",
    "\n",
    "    best_val_intent_acc = 0\n",
    "    best_model_state = None\n",
    "    best_intent_report = \"\"\n",
    "    best_slot_report = \"\"\n",
    "\n",
    "    print(\"ğŸš€ BCEWithLogitsLoss Training Started\")\n",
    "    print(f\"ğŸ“Š Threshold: {threshold}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nğŸ“š Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        # í•™ìŠµ\n",
    "        train_loss = train_epoch_bce(\n",
    "            model, train_loader, optimizer, device,\n",
    "            intent2idx, slot2idx, slot_weights\n",
    "        )\n",
    "\n",
    "        # í‰ê°€\n",
    "        val_intent_acc, intent_report, val_slot_acc, slot_report = evaluate_bce(\n",
    "            model, val_loader, device, intent2idx, slot2idx, threshold\n",
    "        )\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥\n",
    "        train_losses.append(train_loss)\n",
    "        val_intent_accuracies.append(val_intent_acc)\n",
    "        val_slot_accuracies.append(val_slot_acc)\n",
    "\n",
    "        print(f\"ğŸ“‰ Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"ğŸ¯ Val Intent Accuracy: {val_intent_acc:.4f}\")\n",
    "        print(f\"ğŸ· Val Slot Accuracy: {val_slot_acc:.4f}\")\n",
    "\n",
    "        # Best model ì €ì¥\n",
    "        if val_intent_acc > best_val_intent_acc:\n",
    "            best_val_intent_acc = val_intent_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_intent_report = intent_report\n",
    "            best_slot_report = slot_report\n",
    "            print(\"âœ… Best model updated!\")\n",
    "\n",
    "            # ëª¨ë¸ ì €ì¥\n",
    "            if save_path:\n",
    "                import os\n",
    "                import pickle\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                torch.save(best_model_state, os.path.join(save_path, \"best_model.pt\"))\n",
    "                with open(os.path.join(save_path, \"intent2idx.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(intent2idx, f)\n",
    "                with open(os.path.join(save_path, \"slot2idx.pkl\"), \"wb\") as f:\n",
    "                    pickle.dump(slot2idx, f)\n",
    "\n",
    "    print(f\"\\nğŸ‰ Training Completed!\")\n",
    "    print(f\"ğŸ“ˆ Best Intent Accuracy: {best_val_intent_acc:.4f}\")\n",
    "    print(\"\\nğŸ“Š Best Intent Classification Report:\")\n",
    "    print(best_intent_report)\n",
    "    print(\"\\nğŸ“Š Best Slot Classification Report:\")\n",
    "    print(best_slot_report)\n",
    "\n",
    "    return {\n",
    "        'best_model_state': best_model_state,\n",
    "        'train_losses': train_losses,\n",
    "        'val_intent_accuracies': val_intent_accuracies,\n",
    "        'val_slot_accuracies': val_slot_accuracies,\n",
    "        'best_intent_acc': best_val_intent_acc,\n",
    "        'best_intent_report': best_intent_report,\n",
    "        'best_slot_report': best_slot_report\n",
    "    }\n",
    "\n",
    "def plot_training_curves(results, save_path=None):\n",
    "    \"\"\"í•™ìŠµ ê³¡ì„  ì‹œê°í™”\"\"\"\n",
    "    train_losses = results['train_losses']\n",
    "    val_intent_accuracies = results['val_intent_accuracies']\n",
    "    val_slot_accuracies = results['val_slot_accuracies']\n",
    "\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # ì™¼ìª½ Yì¶•: Train Loss\n",
    "    ax1.plot(epochs, train_losses, color='blue', label='Train Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # ì˜¤ë¥¸ìª½ Yì¶•: Accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(epochs, val_intent_accuracies, color='orange', label='Intent Accuracy')\n",
    "    ax2.plot(epochs, val_slot_accuracies, color='green', label='Slot Accuracy')\n",
    "    ax2.set_ylabel('Accuracy', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # ë²”ë¡€\n",
    "    lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines1 + lines2, labels1 + labels2, loc='lower center')\n",
    "\n",
    "    plt.title('ğŸ“Š BCEWithLogitsLoss Training Results')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"ğŸ“Š Plot saved to: {save_path}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "def run_bce_training_pipeline(model_class, save_path=\"best_models/intent-bce-v1\"):\n",
    "    \"\"\"ì „ì²´ BCEWithLogitsLoss í•™ìŠµ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "\n",
    "    print(\"ğŸ”„ 1ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬...\")\n",
    "    df_combined = load_and_preprocess_data()\n",
    "    df_bio = create_bio_dataset(df_combined)\n",
    "\n",
    "    print(\"ğŸ”„ 2ë‹¨ê³„: ë§¤í•‘ ìƒì„±...\")\n",
    "    intent2idx, idx2intent, slot2idx, idx2slot = create_mappings(df_bio)\n",
    "\n",
    "    print(\"ğŸ”„ 3ë‹¨ê³„: ë°ì´í„° ë°¸ëŸ°ì‹±...\")\n",
    "    df_balanced = create_balanced_dataset(df_bio)\n",
    "\n",
    "    print(\"ğŸ”„ 4ë‹¨ê³„: í† í¬ë‚˜ì´ì € ë¡œë“œ...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
    "\n",
    "    print(\"ğŸ”„ 5ë‹¨ê³„: ë°ì´í„°ì…‹ ìƒì„±...\")\n",
    "    train_loader, val_loader = create_datasets(\n",
    "        df_balanced, tokenizer, intent2idx, slot2idx, use_bce=True\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ”„ 6ë‹¨ê³„: ëª¨ë¸ ì´ˆê¸°í™”...\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class(len(intent2idx), len(slot2idx)).to(device)\n",
    "\n",
    "    print(\"ğŸ”„ 7ë‹¨ê³„: í•™ìŠµ ì‹œì‘...\")\n",
    "    results = train_with_bce(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        intent2idx=intent2idx,\n",
    "        slot2idx=slot2idx,\n",
    "        epochs=5,\n",
    "        threshold=0.5,\n",
    "        save_path=save_path\n",
    "    )\n",
    "\n",
    "    print(\"ğŸ”„ 8ë‹¨ê³„: ê²°ê³¼ ì‹œê°í™”...\")\n",
    "    plot_training_curves(results, f\"{save_path}/training_curves.png\" if save_path else None)\n",
    "\n",
    "    return results, intent2idx, slot2idx\n",
    "\n",
    "# ğŸ’¡ ì‚¬ìš© ì˜ˆì‹œ\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ¯ Unified BCEWithLogitsLoss Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    results, intent2idx, slot2idx = run_bce_training_pipeline(KoBERTIntentSlotModel)\n",
    "\n",
    "    print(\"\"\"\n",
    "    ì‚¬ìš©ë²•:\n",
    "    1. ëª¨ë¸ í´ë˜ìŠ¤ import: from your_model import KoBERTIntentSlotModel\n",
    "    2. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰: results, intent2idx, slot2idx = run_bce_training_pipeline(KoBERTIntentSlotModel)\n",
    "\n",
    "    ë˜ëŠ” ë‹¨ê³„ë³„ ì‹¤í–‰:\n",
    "    1. df_combined = load_and_preprocess_data()\n",
    "    2. df_bio = create_bio_dataset(df_combined)\n",
    "    3. intent2idx, idx2intent, slot2idx, idx2slot = create_mappings(df_bio)\n",
    "    4. df_balanced = create_balanced_dataset(df_bio)\n",
    "    5. tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
    "    6. train_loader, val_loader = create_datasets(df_balanced, tokenizer, intent2idx, slot2idx)\n",
    "    7. results = train_with_bce(model, train_loader, val_loader, device, intent2idx, slot2idx)\n",
    "    \"\"\")"
   ],
   "id": "e826fa523a058072"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    " AIRLINE_CODES = [\n",
    "    \"KE\", \"OZ\", \"7C\", \"LJ\", \"BX\", \"ZE\", \"TW\", \"RS\", \"YP\", \"RF\", \"KJ\", \"2I\", \"Q5\",\n",
    "    \"FE\", \"AQ\", \"Y6\", \"GB\", \"K4\", \"YJ\", \"GP\", \"3V\", \"5O\", \"PH\", \"4A\", \"CJ\", \"A0\",\n",
    "    \"B5\", \"8H\", \"H5\", \"2C\", \"OK\", \"DX\", \"L3\", \"ES\", \"D0\", \"Q7\", \"D5\", \"7T\", \"LI\",\n",
    "    \"8D\", \"C0\", \"9A\", \"IV\", \"A5\", \"II\", \"AZ\", \"XE\", \"FK\", \"KL\", \"WA\", \"KK\", \"LV\",\n",
    "    \"GE\", \"LO\", \"WW\", \"7M\", \"M2\", \"OM\", \"MB\", \"6V\", \"MU\", \"2P\", \"PB\", \"CG\", \"OH\",\n",
    "    \"S7\", \"RZ\", \"5S\", \"SK\", \"SL\", \"SP\", \"O3\", \"KI\", \"S0\", \"8F\", \"DT\", \"T0\", \"TP\",\n",
    "    \"TP\", \"YQ\", \"RO\", \"X3\", \"OR\", \"6B\", \"TB\", \"BY\", \"GO\", \"5X\", \"UD\", \"BS\", \"UJ\",\n",
    "    \"RT\", \"V6\", \"2Z\", \"YG\", \"GA\", \"GT\", \"G2\", \"IF\", \"GF\", \"GW\", \"G8\", \"JS\", \"G7\",\n",
    "    \"6G\", \"G3\", \"Y5\", \"G1\", \"5U\", \"GX\", \"CN\", \"YR\", \"GV\", \"HB\", \"Q9\", \"Z5\", \"G6\",\n",
    "    \"5S\", \"GE\", \"ON\", \"NP\", \"NJ\", \"T2\", \"NM\", \"IN\", \"SA\", \"N8\", \"9Y\", \"NA\", \"NE\",\n",
    "    \"NO\", \"RA\", \"1I\", \"NC\", \"Y7\", \"N4\", \"ND\", \"N7\", \"Z0\", \"N0\", \"DY\", \"DY\", \"D8\",\n",
    "    \"NA\", \"O9\", \"NG\", \"VQ\", \"0N\", \"HW\", \"J3\", \"DD\", \"N5\", \"BJ\", \"EJ\", \"7H\", \"OJ\",\n",
    "    \"9J\", \"DN\", \"D3\", \"DL\", \"R6\", \"ZQ\", \"DZ\", \"KB\", \"3R\", \"4Y\", \"DF\", \"LC\", \"B0\",\n",
    "    \"R4\", \"TH\", \"LK\", \"QV\", \"LW\", \"7S\", \"FR\", \"RK\", \"JT\", \"8V\", \"M3\", \"UC\", \"L7\",\n",
    "    \"LA\", \"JJ\", \"XL\", \"LA\", \"4C\", \"PZ\", \"LP\", \"LQ\", \"TM\", \"P7\", \"BN\", \"8L\", \"WZ\",\n",
    "    \"L5\", \"7H\", \"QL\", \"R0\", \"LC\", \"ZL\", \"LM\", \"FV\", \"AT\", \"BI\", \"RW\", \"RJ\", \"3Q\",\n",
    "    \"RG\", \"L8\", \"L9\", \"7R\", \"5R\", \"LH\", \"CL\", \"LH\", \"VL\", \"LG\", \"NJ\", \"GJ\", \"LT\",\n",
    "    \"WB\", \"YL\", \"LN\", \"5U\", \"4P\", \"EX\", \"8N\", \"F2\", \"YX\", \"L2\", \"L0\", \"RI\", \"FC\",\n",
    "    \"UJ\", \"DI\", \"O8\", \"MP\", \"4M\", \"M7\", \"MY\", \"2M\", \"2Y\", \"N7\", \"C6\", \"LL\", \"5G\",\n",
    "    \"MJ\", \"9T\", \"W5\", \"6M\", \"AE\", \"7Y\", \"NR\", \"3W\", \"DB\", \"MH\", \"VM\", \"DJ\", \"4X\",\n",
    "    \"N5\", \"BM\", \"YV\", \"WD\", \"L6\", \"Q2\", \"MT\", \"XF\", \"ME\", \"M4\", \"UB\", \"8M\", \"K7\",\n",
    "    \"J4\", \"OD\", \"ID\", \"UP\", \"PG\", \"V9\", \"VK\", \"QH\", \"NB\", \"2T\", \"J8\", \"RR\", \"VS\",\n",
    "    \"VA\", \"XR\", \"BD\", \"J0\", \"8E\", \"CH\", \"3B\", \"JV\", \"JD\", \"VN\", \"0V\", \"V4\", \"B2\",\n",
    "    \"AB\", \"VI\", \"N3\", \"Q6\", \"Y4\", \"V7\", \"OB\", \"2L\", \"U4\", \"RP\", \"UZ\", \"VY\", \"B3\",\n",
    "    \"4B\", \"LB\", \"FB\", \"TF\", \"NT\", \"TT\", \"1X\", \"SN\", \"MX\", \"SI\", \"BV\", \"BZ\", \"BO\",\n",
    "    \"BZ\", \"BG\", \"VB\", \"BH\", \"UK\", \"V4\", \"VJ\", \"VU\", \"B4\", \"UQ\", \"NT\", \"VP\", \"OL\",\n",
    "    \"OL\", \"SV\", \"S1\", \"WN\", \"2S\", \"9R\", \"F2\", \"ZF\", \"FA\", \"SC\", \"OV\", \"SO\", \"FM\",\n",
    "    \"MF\", \"SH\", \"CE\", \"IH\", \"9X\", \"PL\", \"SY\", \"R8\", \"SR\", \"2U\", \"S6\", \"2R\", \"YH\",\n",
    "    \"EZ\", \"WG\", \"ZH\", \"DK\", \"ER\", \"D2\", \"5J\", \"DG\", \"C2\", \"K3\", \"PV\", \"IS\", \"9M\",\n",
    "    \"5Z\", \"SZ\", \"6J\", \"FW\", \"IE\", \"SP\", \"SK\", \"DR\", \"SD\", \"SJ\", \"PY\", \"Y8\", \"N9\",\n",
    "    \"IU\", \"UL\", \"5N\", \"6Y\", \"2N\", \"QS\", \"6D\", \"3Z\", \"7O\", \"2E\", \"LX\", \"WT\", \"Y3\",\n",
    "    \"ML\", \"3E\", \"BQ\", \"ZA\", \"GQ\", \"H2\", \"RD\", \"H8\", \"UY\", \"S8\", \"U3\", \"QU\", \"GG\",\n",
    "    \"BC\", \"PQ\", \"U5\", \"OW\", \"OO\", \"LC\", \"M8\", \"TE\", \"QN\", \"DO\", \"DV\", \"TR\", \"HK\",\n",
    "    \"S5\", \"2I\", \"JX\", \"4E\", \"4R\", \"7G\", \"SG\", \"SG\", \"P8\", \"NK\", \"C7\", \"YR\", \"5G\",\n",
    "    \"RB\", \"BB\", \"SI\", \"A2\", \"QG\", \"WX\", \"K5\", \"3M\", \"US\", \"7L\", \"ZP\", \"7E\", \"SQ\",\n",
    "    \"SQ\", \"XQ\", \"3U\", \"XO\", \"EH\", \"DM\", \"AG\", \"6A\", \"JI\", \"AR\", \"FG\", \"W3\", \"Z8\",\n",
    "    \"MZ\", \"A8\", \"M6\", \"AA\", \"8R\", \"XP\", \"YK\", \"4B\", \"ZR\", \"GU\", \"A0\", \"2K\", \"TA\",\n",
    "    \"WC\", \"QT\", \"LR\", \"AV\", \"9V\", \"X9\", \"X8\", \"JU\", \"4K\", \"KP\", \"0A\", \"8V\", \"P9\",\n",
    "    \"KP\", \"GM\", \"AM\", \"6R\", \"5D\", \"E4\", \"SU\", \"FW\", \"FI\", \"Q7\", \"F7\", \"2O\", \"I4\",\n",
    "    \"J2\", \"S4\", \"ZF\", \"AD\", \"2F\", \"AJ\", \"QP\", \"AK\", \"V8\", \"5Y\", \"RC\", \"J7\", \"AW\",\n",
    "    \"XU\", \"BU\", \"8U\", \"6L\", \"O4\", \"KO\", \"J5\", \"AS\", \"JN\", \"G4\", \"DQ\", \"6R\", \"KH\",\n",
    "    \"VC\", \"QQ\", \"UJ\", \"AP\", \"2B\", \"G0\", \"5A\", \"2G\", \"Q3\", \"A2\", \"IZ\", \"YE\", \"YC\",\n",
    "    \"R3\", \"AN\", \"4W\", \"9I\", \"UE\", \"A3\", \"WK\", \"B8\", \"EA\", \"ET\", \"EK\", \"BR\", \"8K\",\n",
    "    \"5V\", \"GD\", \"ES\", \"GT\", \"GL\", \"4N\", \"PX\", \"NZ\", \"EN\", \"HD\", \"RM\", \"GZ\", \"7I\",\n",
    "    \"LZ\", \"ZM\", \"MD\", \"NX\", \"KM\", \"MV\", \"MK\", \"4O\", \"NF\", \"KF\", \"BP\", \"2J\", \"C7\",\n",
    "    \"PJ\", \"HC\", \"JU\", \"HM\", \"Y2\", \"GI\", \"PF\", \"G9\", \"3O\", \"3L\", \"E5\", \"KC\", \"2A\",\n",
    "    \"NY\", \"ZB\", \"6I\", \"AH\", \"TZ\", \"CC\", \"UU\", \"3N\", \"N6\", \"ZW\", \"X5\", \"UX\", \"3H\",\n",
    "    \"AI\", \"IX\", \"9H\", \"CA\", \"CA\", \"DJ\", \"UM\", \"3C\", \"3E\", \"2Q\", \"TX\", \"TY\", \"AC\",\n",
    "    \"RV\", \"AC\", \"KS\", \"XK\", \"HF\", \"YN\", \"IK\", \"VT\", \"TN\", \"ST\", \"TC\", \"TS\", \"A6\",\n",
    "    \"8C\", \"6C\", \"8T\", \"7P\", \"FS\", \"AF\", \"F4\", \"P4\", \"HT\", \"LD\", \"AO\", \"TL\", \"KA\",\n",
    "    \"8G\", \"N2\", \"NL\", \"3S\", \"M0\", \"V5\", \"KW\", \"2S\", \"7L\", \"F5\", \"ZV\", \"XZ\", \"K2\",\n",
    "    \"EI\", \"EI\", \"4Z\", \"X8\", \"BT\", \"4Y\", \"RU\", \"PA\", \"T6\", \"AK\", \"D7\", \"ED\", \"NQ\",\n",
    "    \"JK\", \"EF\", \"SM\", \"SB\", \"P2\", \"RE\", \"ZD\", \"VF\", \"8J\", \"ZU\", \"ET\", \"EY\", \"EE\",\n",
    "    \"9E\", \"MQ\", \"E4\", \"G4\", \"7Q\", \"EL\", \"LY\", \"BS\", \"BA\", \"IY\", \"YT\", \"O7\", \"HZ\",\n",
    "    \"6O\", \"GR\", \"OC\", \"OG\", \"UI\", \"WY\", \"OF\", \"OS\", \"YI\", \"BK\", \"OA\", \"OY\", \"EB\",\n",
    "    \"WP\", \"Q9\", \"R5\", \"UR\", \"U6\", \"UQ\", \"HY\", \"PS\", \"4W\", \"OX\", \"WL\", \"3G\", \"2W\",\n",
    "    \"3P\", \"KD\", \"WU\", \"PN\", \"WS\", \"WR\", \"WC\", \"WF\", \"W6\", \"W4\", \"5W\", \"W9\", \"7W\",\n",
    "    \"WH\", \"P5\", \"IW\", \"UA\", \"B7\", \"U7\", \"6U\", \"QY\", \"H6\", \"PS\", \"YU\", \"Q4\", \"E6\",\n",
    "    \"EW\", \"YZ\", \"UF\", \"UT\", \"RF\", \"X7\", \"H7\", \"IA\", \"EP\", \"B9\", \"IR\", \"IO\", \"E9\",\n",
    "    \"I2\", \"YW\", \"IB\", \"QI\", \"E7\", \"T3\", \"I8\", \"MG\", \"RD\", \"7Z\", \"U2\", \"DS\", \"EC\",\n",
    "    \"MS\", \"EO\", \"E7\", \"XN\", \"7A\", \"QE\", \"QZ\", \"6E\", \"I7\", \"JY\", \"I4\", \"8B\", \"V8\",\n",
    "    \"IJ\", \"JL\", \"JC\", \"KZ\", \"JO\", \"J9\", \"JM\", \"ZN\", \"RY\", \"JG\", \"QK\", \"NU\", \"LC\",\n",
    "    \"NH\", \"JL\", \"ZK\", \"B6\", \"WJ\", \"JA\", \"JZ\", \"3K\", \"GK\", \"JQ\", \"4J\", \"LS\", \"JO\",\n",
    "    \"JR\", \"A9\", \"GH\", \"6J\", \"3J\", \"IM\", \"HO\", \"CZ\", \"CZ\", \"CO\", \"MU\", \"G5\", \"I9\",\n",
    "    \"CK\", \"CI\", \"XM\", \"D4\", \"ZG\", \"9D\", \"TZ\", \"KN\", \"CF\", \"VC\", \"6Q\", \"X7\", \"5C\",\n",
    "    \"GS\", \"HT\", \"EU\", \"GM\", \"9C\", \"OQ\", \"QW\", \"C3\", \"6L\", \"CV\", \"C8\", \"W8\", \"PM\",\n",
    "    \"HH\", \"8F\", \"NV\", \"V3\", \"BW\", \"QC\", \"Z7\", \"9Q\", \"IQ\", \"QR\", \"VR\", \"A7\", \"K4\",\n",
    "    \"K9\", \"MO\", \"RQ\", \"K6\", \"KR\", \"5T\", \"AU\", \"CX\", \"C5\", \"6C\", \"LF\", \"GY\", \"HQ\",\n",
    "    \"KQ\", \"QB\", \"8K\", \"KX\", \"9K\", \"M5\", \"4K\", \"CD\", \"XC\", \"XR\", \"SS\", \"KO\", \"GW\",\n",
    "    \"CQ\", \"7C\", \"CM\", \"P5\", \"FC\", \"DE\", \"V0\", \"C4\", \"8Z\", \"QF\", \"CU\", \"KU\", \"KY\",\n",
    "    \"QO\", \"QA\", \"KV\", \"C8\", \"OU\", \"VE\", \"KG\", \"FK\", \"Z3\", \"TB\", \"3T\", \"SF\", \"SL\",\n",
    "    \"VZ\", \"FD\", \"XJ\", \"TG\", \"IT\", \"ZT\", \"TM\", \"HJ\", \"K3\", \"TK\", \"TI\", \"TQ\", \"BV\",\n",
    "    \"T5\", \"T9\", \"U8\", \"TU\", \"UG\", \"M8\", \"T7\", \"8B\", \"HV\", \"TO\", \"R2\", \"Q8\", \"T7\",\n",
    "    \"C3\", \"TJ\", \"9N\", \"IL\", \"TD\", \"T7\", \"TV\", \"5P\", \"ZP\", \"P6\", \"FY\", \"PK\", \"HP\",\n",
    "    \"OP\", \"8Y\", \"P1\", \"8P\", \"BL\", \"PC\", \"7V\", \"FX\", \"FN\", \"UF\", \"DP\", \"PD\", \"NI\",\n",
    "    \"PO\", \"PI\", \"Z4\", \"FU\", \"6P\", \"BF\", \"P0\", \"F9\", \"FS\", \"FA\", \"4F\", \"E3\", \"3X\",\n",
    "    \"MI\", \"FH\", \"P6\", \"PW\", \"SX\", \"WV\", \"XY\", \"FL\", \"5M\", \"IF\", \"S9\", \"G6\", \"EQ\",\n",
    "    \"8W\", \"F0\", \"5F\", \"FT\", \"9P\", \"FP\", \"FZ\", \"FO\", \"FA\", \"F3\", \"3F\", \"F6\", \"YS\",\n",
    "    \"D3\", \"PU\", \"F8\", \"OG\", \"W2\", \"PT\", \"FJ\", \"MM\", \"PE\", \"AY\", \"HI\", \"Z2\", \"PR\",\n",
    "    \"YB\", \"HA\", \"5K\", \"3L\", \"HU\", \"HG\", \"H4\", \"H7\", \"HR\", \"NS\", \"HN\", \"H3\", \"H3\",\n",
    "    \"JB\", \"2L\", \"QX\", \"5Q\", \"HD\", \"UO\", \"HX\", \"RH\", \"RS\", \"WI\", \"JH\", \"MR\", \"HJ\",\n",
    "    \"H9\", \"OI\",\n",
    "\n",
    "    \"HL\" # í•™ìŠµë°ì´í„°ì— ìˆëŠ” ê°€ì§œ í•­ê³µí¸ ì½”ë“œ\n",
    "]"
   ],
   "id": "9dee68e53b542ba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    KoBERT ê¸°ë°˜ ì „ì²˜ë¦¬ì— ì í•©í•˜ë„ë¡ íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "    \"\"\"\n",
    "    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±ë§Œ ë‚¨ê¸°ê¸°\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A3a-zA-Z0-9\\s]\", \"\", str(text))\n",
    "    # ë‹¤ì¤‘ ê³µë°± ì œê±°\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# í”Œë ˆì´ìŠ¤í™€ë”\n",
    "FLIGHT_PREFIX = \"FLIGHT\"   # í† í°ì€ âŸªFLIGHT0âŸ«, âŸªFLIGHT1âŸ« ... í˜•íƒœë¡œ ìƒì„±\n",
    "TERMINAL_PREFIX = \"TERMINAL\"  # í† í°ì€ âŸªTERMINAL0âŸ«, âŸªTERMINAL1âŸ« ... í˜•íƒœë¡œ ìƒì„±\n",
    "# ìœ íš¨í•œ í•­ê³µì‚¬ ì½”ë“œë§Œ ë§¤ì¹­í•˜ëŠ” íŒ¨í„´ ìƒì„±\n",
    "airline_codes_pattern = '|'.join(AIRLINE_CODES)\n",
    "\n",
    "# ì¼ë°˜ì ì¸ í•­ê³µí¸ íŒ¨í„´ (ê³µë°± ì—†ìŒ)\n",
    "flight_pattern_normal = re.compile(rf'\\b({airline_codes_pattern})\\s*[-]?\\s*(\\d{{1,4}})\\b', re.IGNORECASE)\n",
    "\n",
    "# ë„ì–´ì“°ê¸°ëœ í•­ê³µí¸ íŒ¨í„´ (ì˜ˆ: \"HL 7201\", \"7 C 0102\")\n",
    "flight_pattern_spaced = re.compile(r'\\b([A-Za-z0-9])\\s+([A-Za-z0-9])\\s+(\\d{1,4})\\b', re.IGNORECASE)\n",
    "\n",
    "def _collapse_flight_spans(text: str) -> str:\n",
    "    \"\"\"í•­ê³µí¸ í‘œí˜„ì„ í•­ìƒ ë¶™ì—¬ì“°ê¸°(í•˜ì´í”ˆ/ê³µë°± ì œê±°) + ëŒ€ë¬¸ìë¡œ í†µì¼.\"\"\"\n",
    "    # ì¼ë°˜ íŒ¨í„´ ì²˜ë¦¬ (ke 907 -> KE907, KE 907 -> KE907)\n",
    "    text = flight_pattern_normal.sub(lambda m: (m.group(1) + m.group(2)).upper(), text)\n",
    "\n",
    "    # ë„ì–´ì“°ê¸°ëœ íŒ¨í„´ ì²˜ë¦¬ (7 c 0102 -> 7C0102, hl 7201 -> HL7201)\n",
    "    def spaced_replacer(m):\n",
    "        code = m.group(1) + m.group(2)  # í•­ê³µì‚¬ ì½”ë“œ ê²°í•©\n",
    "        number = m.group(3)             # í•­ê³µí¸ ë²ˆí˜¸\n",
    "        # ìœ íš¨í•œ í•­ê³µì‚¬ ì½”ë“œì¸ì§€ í™•ì¸\n",
    "        if code.upper() in AIRLINE_CODES:\n",
    "            return (code + number).upper()  # ëŒ€ë¬¸ìë¡œ ë³€í™˜\n",
    "        return m.group(0)  # ë§¤ì¹­ë˜ì§€ ì•Šìœ¼ë©´ ì›ë³¸ ìœ ì§€\n",
    "\n",
    "    text = flight_pattern_spaced.sub(spaced_replacer, text)\n",
    "    return text\n",
    "\n",
    "def _collapse_terminal_spans(text: str) -> str:\n",
    "    \"\"\"í„°ë¯¸ë„ í‘œí˜„ì„ T1, T2ë¡œ ì •ê·œí™”.\"\"\"\n",
    "    # T1 ê´€ë ¨ íŒ¨í„´ë“¤\n",
    "    t1_patterns = [\n",
    "        r'(?:ì œ?\\s*1\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„|í„°ë¯¸ë„\\s*1|T\\s*-?\\s*1|ì²«\\s*ë²ˆ?\\s*ì§¸\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„|ì œì¼\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„)',\n",
    "        r'(?:ì¼\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„|í„°ë¯¸ë„\\s*ì¼)',\n",
    "        r'(?:ì œ\\s*1\\s*ì—¬ê°\\s*í„°ë¯¸ë„|ì œ1\\s*ì—¬ê°\\s*í„°ë¯¸ë„)',\n",
    "    ]\n",
    "\n",
    "    # T2 ê´€ë ¨ íŒ¨í„´ë“¤\n",
    "    t2_patterns = [\n",
    "        r'(?:ì œ?\\s*2\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„|í„°ë¯¸ë„\\s*2|T\\s*-?\\s*2|ë‘\\s*ë²ˆ?\\s*ì§¸\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„|ì œì´\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„)',\n",
    "        r'(?:ì´\\s*(?:ì—¬ê°\\s*)?í„°ë¯¸ë„|í„°ë¯¸ë„\\s*ì´)',\n",
    "        r'(?:ì œ\\s*2\\s*ì—¬ê°\\s*í„°ë¯¸ë„|ì œ2\\s*ì—¬ê°\\s*í„°ë¯¸ë„)',\n",
    "    ]\n",
    "\n",
    "    # T1ìœ¼ë¡œ ì •ê·œí™”\n",
    "    for pattern in t1_patterns:\n",
    "        text = re.sub(pattern, 'T1', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # T2ë¡œ ì •ê·œí™”\n",
    "    for pattern in t2_patterns:\n",
    "        text = re.sub(pattern, 'T2', text, flags=re.IGNORECASE)\n",
    "\n",
    "    return text\n",
    "\n",
    "_FACILITY_LIST = [\"ê¸°ë„ì‹¤\", \"ê²€ì—­ì¥\", \"ìˆ˜ìœ ì‹¤\"]  # í•„ìš”í•˜ë©´ ì—¬ê¸°ì— ê³„ì† ì¶”ê°€\n",
    "def _collapse_keyword(text: str, word: str) -> str:\n",
    "    base = word.replace(\" \", \"\")\n",
    "    # í•œê¸€/ì˜ë¬¸/ìˆ«ì ê²½ê³„ì—ì„œë§Œ ë§¤ì¹˜ë˜ê²Œ ê²½ê³„ ì¶”ê°€\n",
    "    pattern = r'(?<![ê°€-í£A-Za-z0-9])' + r'\\s*'.join(map(re.escape, base)) + r'(?![ê°€-í£A-Za-z0-9])'\n",
    "    return re.sub(pattern, base, text)\n",
    "\n",
    "def _collapse_facility_spans(text: str) -> str:\n",
    "    for w in _FACILITY_LIST:\n",
    "        text = _collapse_keyword(text, w)\n",
    "    return text\n",
    "\n",
    "def normalize_with_morph(text: str) -> str:\n",
    "    # 0) íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬\n",
    "    processed_text = clean_text(text)\n",
    "\n",
    "    # 1) í•­ê³µí¸ì„ ë¨¼ì € ë¶™ì—¬ì“°ê¸° ì •ê·œí™” (KE 907 -> KE907)\n",
    "    processed_text = _collapse_flight_spans(processed_text)\n",
    "\n",
    "    # 1.5) í„°ë¯¸ë„ í‘œí˜„ ì •ê·œí™” (1í„°ë¯¸ë„ -> T1, ì œ2í„°ë¯¸ë„ -> T2)\n",
    "    processed_text = _collapse_terminal_spans(processed_text)\n",
    "\n",
    "    # 2) í•­ê³µí¸ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì¹˜í™˜ (ì—¬ëŸ¬ ê°œ ì§€ì›)\n",
    "    flight_map = {}  # ì˜ˆ: {'âŸªFLIGHT0âŸ«': 'KE907', 'âŸªFLIGHT1âŸ«': 'VS5501'}\n",
    "    flight_counter = 0\n",
    "    def _flight_repl(m):\n",
    "        nonlocal flight_counter\n",
    "        code = (m.group(1) + m.group(2)).upper()     # ë¶™ì—¬ì“°ê¸° + ëŒ€ë¬¸ì ë³€í™˜\n",
    "        token = f'âŸª{FLIGHT_PREFIX}{flight_counter}âŸ«'\n",
    "        flight_map[token] = code\n",
    "        flight_counter += 1\n",
    "        return token\n",
    "\n",
    "    processed_text = flight_pattern_normal.sub(_flight_repl, processed_text)\n",
    "\n",
    "    # 2.5) í„°ë¯¸ë„ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì¹˜í™˜\n",
    "    terminal_map = {}  # ì˜ˆ: {'âŸªTERMINAL0âŸ«': 'T1', 'âŸªTERMINAL1âŸ«': 'T2'}\n",
    "    terminal_counter = 0\n",
    "    def _terminal_repl(m):\n",
    "        nonlocal terminal_counter\n",
    "        terminal_code = m.group(0)  # T1 ë˜ëŠ” T2\n",
    "        token = f'âŸª{TERMINAL_PREFIX}{terminal_counter}âŸ«'\n",
    "        terminal_map[token] = terminal_code\n",
    "        terminal_counter += 1\n",
    "        return token\n",
    "\n",
    "    # T1, T2 íŒ¨í„´ì„ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ì¹˜í™˜\n",
    "    terminal_pattern = re.compile(r'\\bT[12]\\b')\n",
    "    processed_text = terminal_pattern.sub(_terminal_repl, processed_text)\n",
    "\n",
    "\n",
    "    # 3) í˜•íƒœì†Œ ë¶„ì„ (ì •ê·œí™”/ì–´ê°„í™” ë”)\n",
    "    tokens = okt.morphs(processed_text, norm=False, stem=False)\n",
    "\n",
    "    # 4) ë‹¤ì‹œ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°\n",
    "    text_after = \" \".join(tokens)\n",
    "\n",
    "    # 5) í”Œë ˆì´ìŠ¤í™€ë” ë³µì›\n",
    "    #    í† í¬ë‚˜ì´ì¦ˆê°€ ê³µë°±ì„ ë¼ì›Œë„£ì–´ë„(âŸª FLIGHT 0 âŸ«) ì •í™•íˆ ë³µì›ë˜ë„ë¡ ì²˜ë¦¬\n",
    "    for token, code in flight_map.items():\n",
    "        core = token[1:-1]  # 'FLIGHT0'\n",
    "        m = re.match(r'(FLIGHT)(\\d+)$', core)\n",
    "        if m:\n",
    "            pat = re.compile(r'âŸª\\s*' + m.group(1) + r'\\s*' + m.group(2) + r'\\s*âŸ«')\n",
    "            text_after = pat.sub(code, text_after)\n",
    "        # í˜¹ì‹œ ê·¸ëŒ€ë¡œ ë‚¨ì•„ìˆìœ¼ë©´ ì§ì ‘ ì¹˜í™˜\n",
    "        text_after = text_after.replace(token, code)\n",
    "\n",
    "    # í„°ë¯¸ë„ í”Œë ˆì´ìŠ¤í™€ë” ë³µì›\n",
    "    for token, code in terminal_map.items():\n",
    "        core = token[1:-1]  # 'TERMINAL0'\n",
    "        m = re.match(r'(TERMINAL)(\\d+)$', core)\n",
    "        if m:\n",
    "            pat = re.compile(r'âŸª\\s*' + m.group(1) + r'\\s*' + m.group(2) + r'\\s*âŸ«')\n",
    "            text_after = pat.sub(code, text_after)\n",
    "        # í˜¹ì‹œ ê·¸ëŒ€ë¡œ ë‚¨ì•„ìˆìœ¼ë©´ ì§ì ‘ ì¹˜í™˜\n",
    "        text_after = text_after.replace(token, code)\n",
    "\n",
    "    # 6) í˜¹ì‹œ ë‚¨ì€ ê³µë°±/í•˜ì´í”ˆ ë³€í˜•ì„ ë‹¤ì‹œ í•œ ë²ˆ ì •ê·œí™”\n",
    "    text_after = _collapse_flight_spans(text_after)\n",
    "    text_after = _collapse_terminal_spans(text_after)\n",
    "\n",
    "    text_after = _collapse_facility_spans(text_after)\n",
    "\n",
    "    # 7) ê³µë°± ì •ë¦¬\n",
    "    text_after = re.sub(r'\\s+', ' ', text_after).strip()\n",
    "    return text_after\n"
   ],
   "id": "d3722fa4c058892e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax, sigmoid\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# ğŸ”§ ì„¤ì • ë° ê²½ë¡œ\n",
    "INTENT2IDX_PATH = \"best_models/intent-bce-v1/intent2idx.pkl\"\n",
    "SLOT2IDX_PATH = \"best_models/intent-bce-v1/slot2idx.pkl\"\n",
    "MODEL_PATH = \"best_models/intent-bce-v1/best_model.pt\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ğŸ—ï¸ ëª¨ë¸ í´ë˜ìŠ¤ (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "class KoBERTIntentSlotModel(nn.Module):\n",
    "    def __init__(self, num_intents, num_slots):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"skt/kobert-base-v1\")\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "\n",
    "        self.intent_classifier = nn.Linear(hidden_size, num_intents)\n",
    "        self.slot_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_slots)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        intent_logits = self.intent_classifier(pooled_output)\n",
    "        slot_logits = self.slot_classifier(sequence_output)\n",
    "\n",
    "        return intent_logits, slot_logits\n",
    "\n",
    "# âœ… ì¸ë±ìŠ¤ ë§µ ë° ëª¨ë¸ ë¡œë”©\n",
    "def load_model_and_mappings():\n",
    "    \"\"\"ëª¨ë¸ê³¼ ë§¤í•‘ ì •ë³´ ë¡œë“œ\"\"\"\n",
    "    # ì¸ë±ìŠ¤ ë§µ ë¡œë”©\n",
    "    with open(INTENT2IDX_PATH, \"rb\") as f:\n",
    "        intent2idx = pickle.load(f)\n",
    "    with open(SLOT2IDX_PATH, \"rb\") as f:\n",
    "        slot2idx = pickle.load(f)\n",
    "\n",
    "    idx2intent = {v: k for k, v in intent2idx.items()}\n",
    "    idx2slot = {v: k for k, v in slot2idx.items()}\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    model = KoBERTIntentSlotModel(num_intents=len(intent2idx), num_slots=len(slot2idx))\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"skt/kobert-base-v1\", use_fast=False)\n",
    "\n",
    "    return model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot\n",
    "\n",
    "# ğŸ§± í† í° â†’ ë‹¨ì–´ ë³‘í•© + ìŠ¬ë¡¯ ì •ë ¬\n",
    "def merge_tokens_and_slots(tokens, slot_ids, idx2slot):\n",
    "    \"\"\"í† í°ê³¼ ìŠ¬ë¡¯ ë³‘í•©\"\"\"\n",
    "    merged = []\n",
    "    word = ''\n",
    "    current_slot = ''\n",
    "\n",
    "    for token, slot_id in zip(tokens, slot_ids):\n",
    "        slot = idx2slot.get(slot_id, 'O')\n",
    "\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "\n",
    "        if token.startswith(\"â–\"):  # ìƒˆ ë‹¨ì–´ ì‹œì‘\n",
    "            if word:\n",
    "                merged.append((word, current_slot))\n",
    "            word = token[1:]\n",
    "            current_slot = slot\n",
    "        else:\n",
    "            word += token.replace(\"â–\", \"\")\n",
    "\n",
    "    if word:\n",
    "        merged.append((word, current_slot))\n",
    "\n",
    "    return merged\n",
    "\n",
    "# ğŸ”® BCEWithLogitsLoss ê¸°ë°˜ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "def predict_with_bce(text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                     threshold=0.5, top_k_intents=3, max_length=64):\n",
    "    \"\"\"\n",
    "    BCEWithLogitsLossë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ìœ„í•œ ì˜ˆì¸¡ í•¨ìˆ˜\n",
    "\n",
    "    Args:\n",
    "        text: ì…ë ¥ í…ìŠ¤íŠ¸\n",
    "        threshold: Intent ë¶„ë¥˜ ì„ê³„ê°’ (default: 0.5)\n",
    "        top_k_intents: ìƒìœ„ Kê°œ ì¸í…íŠ¸ ë°˜í™˜ (default: 3)\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        intent_logits, slot_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # Intent ì˜ˆì¸¡ (Sigmoid ê¸°ë°˜)\n",
    "        intent_probs = sigmoid(intent_logits)[0]  # [num_intents]\n",
    "\n",
    "        # ì„ê³„ê°’ ì´ìƒì˜ ì¸í…íŠ¸ë“¤ ì°¾ê¸°\n",
    "        high_confidence_intents = []\n",
    "        for i, prob in enumerate(intent_probs):\n",
    "            if prob.item() >= threshold:\n",
    "                intent_name = idx2intent[i]\n",
    "                high_confidence_intents.append((intent_name, prob.item()))\n",
    "\n",
    "        # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "        high_confidence_intents.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # ë§Œì•½ ì„ê³„ê°’ ì´ìƒì¸ ê²Œ ì—†ë‹¤ë©´ ìµœê³  í™•ë¥  í•˜ë‚˜ë§Œ\n",
    "        if not high_confidence_intents:\n",
    "            max_idx = torch.argmax(intent_probs).item()\n",
    "            max_prob = intent_probs[max_idx].item()\n",
    "            high_confidence_intents = [(idx2intent[max_idx], max_prob)]\n",
    "\n",
    "        # Top-K ì¸í…íŠ¸ (ì „ì²´ ìˆœìœ„ìš©)\n",
    "        topk_probs, topk_indices = torch.topk(intent_probs, min(top_k_intents, len(intent2idx)))\n",
    "        all_top_intents = [(idx2intent[idx.item()], prob.item())\n",
    "                          for idx, prob in zip(topk_indices, topk_probs)]\n",
    "\n",
    "        # ìŠ¬ë¡¯ ì˜ˆì¸¡ (ê¸°ì¡´ê³¼ ë™ì¼ - Softmax ê¸°ë°˜)\n",
    "        slot_pred_ids = torch.argmax(slot_logits, dim=2)[0].tolist()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        merged_slots = merge_tokens_and_slots(tokens, slot_pred_ids, idx2slot)\n",
    "\n",
    "    return {\n",
    "        'high_confidence_intents': high_confidence_intents,  # ì„ê³„ê°’ ì´ìƒ\n",
    "        'all_top_intents': all_top_intents,                  # ì „ì²´ Top-K\n",
    "        'slots': merged_slots,\n",
    "        'is_multi_intent': len(high_confidence_intents) > 1,\n",
    "        'max_intent_prob': max(prob for _, prob in all_top_intents),\n",
    "        'intent_probs_raw': intent_probs.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# ğŸ¯ ë¼ìš°íŒ… ê²°ì • í•¨ìˆ˜ (3êµ¬ê°„ ì„ê³„ê°’)\n",
    "def make_routing_decision(text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                         tau_hi=0.8, tau_lo=0.3, multi_threshold=0.5):\n",
    "    \"\"\"\n",
    "    3êµ¬ê°„ ì„ê³„ê°’ ê¸°ë°˜ ë¼ìš°íŒ… ê²°ì •\n",
    "\n",
    "    Args:\n",
    "        tau_hi: ë†’ì€ ì„ê³„ê°’ (ë°”ë¡œ ë¼ìš°íŒ…)\n",
    "        tau_lo: ë‚®ì€ ì„ê³„ê°’ (gray zone)\n",
    "        multi_threshold: ë³µí•© ì˜ë„ íŒë‹¨ ì„ê³„ê°’\n",
    "    \"\"\"\n",
    "    result = predict_with_bce(\n",
    "        text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "        threshold=multi_threshold\n",
    "    )\n",
    "\n",
    "    max_prob = result['max_intent_prob']\n",
    "    is_multi = result['is_multi_intent']\n",
    "\n",
    "    # ë³µí•© ì˜ë„ì¸ ê²½ìš°\n",
    "    if is_multi:\n",
    "        decision = \"multi_intent\"\n",
    "        action = f\"ğŸ§  ë©”ì¸ LLM ì²˜ë¦¬: ë³µí•© ì˜ë„ ({len(result['high_confidence_intents'])}ê°œ)\"\n",
    "        llm_type = \"main\"\n",
    "    # ë‹¨ì¼ ì˜ë„ + ë†’ì€ ì‹ ë¢°ë„\n",
    "    elif max_prob >= tau_hi:\n",
    "        decision = \"route\"\n",
    "        top_intent = result['all_top_intents'][0][0]\n",
    "        action = f\"âœ… ì§ì ‘ ë¼ìš°íŒ…: {top_intent} í•¸ë“¤ëŸ¬ í˜¸ì¶œ\"\n",
    "        llm_type = None\n",
    "    # ë‹¨ì¼ ì˜ë„ + ë‚®ì€ ì‹ ë¢°ë„\n",
    "    else:\n",
    "        decision = \"abstain\"\n",
    "        action = \"ğŸ§  ë©”ì¸ LLM ì²˜ë¦¬: ì‹ ë¢°ë„ ë‚®ìŒ, ì „ì²´ ì˜ë„ ë¶„ì„ í•„ìš”\"\n",
    "        llm_type = \"main\"\n",
    "\n",
    "    return {\n",
    "        'decision': decision,\n",
    "        'action': action,\n",
    "        'llm_type': llm_type,\n",
    "        'confidence': max_prob,\n",
    "        'intents': result['high_confidence_intents'],\n",
    "        'all_intents': result['all_top_intents'],\n",
    "        'slots': result['slots'],\n",
    "        'is_multi_intent': is_multi\n",
    "    }\n",
    "\n",
    "# ğŸ” ìƒì„¸ ë¶„ì„ í•¨ìˆ˜\n",
    "def analyze_prediction(text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                      threshold=0.5, show_all_probs=False):\n",
    "    \"\"\"ìƒì„¸í•œ ì˜ˆì¸¡ ë¶„ì„\"\"\"\n",
    "    result = predict_with_bce(\n",
    "        text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "        threshold=threshold\n",
    "    )\n",
    "\n",
    "    print(f\"\\nğŸ“ ì…ë ¥: {text}\")\n",
    "    print(f\"ğŸ¯ ì„ê³„ê°’: {threshold}\")\n",
    "    print(f\"ğŸ”¢ ë³µí•© ì˜ë„ ì—¬ë¶€: {'Yes' if result['is_multi_intent'] else 'No'}\")\n",
    "\n",
    "    print(f\"\\nğŸ† ì„ê³„ê°’ ì´ìƒ ì¸í…íŠ¸ ({len(result['high_confidence_intents'])}ê°œ):\")\n",
    "    for i, (intent, prob) in enumerate(result['high_confidence_intents'], 1):\n",
    "        print(f\"   {i}. {intent}: {prob:.4f}\")\n",
    "\n",
    "    print(f\"\\nğŸ“Š ì „ì²´ Top-{len(result['all_top_intents'])} ì¸í…íŠ¸:\")\n",
    "    for i, (intent, prob) in enumerate(result['all_top_intents'], 1):\n",
    "        print(f\"   {i}. {intent}: {prob:.4f}\")\n",
    "\n",
    "    print(f\"\\nğŸ­ ìŠ¬ë¡¯ íƒœê¹… ê²°ê³¼:\")\n",
    "    for word, slot in result['slots']:\n",
    "        print(f\"   - {word}: {slot}\")\n",
    "\n",
    "    if result['is_multi_intent']:\n",
    "        print(f\"\\nğŸ¯ ë³µí•© ì˜ë„ ê°ì§€ë¨!\")\n",
    "\n",
    "    return result\n",
    "\n",
    "# ğŸ§ª ì¸í„°ë™í‹°ë¸Œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜\n",
    "def interactive_test():\n",
    "    \"\"\"ì¸í„°ë™í‹°ë¸Œ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "    print(\"ğŸš€ BCEWithLogitsLoss ê¸°ë°˜ ì¸í…íŠ¸/ìŠ¬ë¡¯ ì˜ˆì¸¡ê¸°\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ\n",
    "    print(\"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘...\")\n",
    "    model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot = load_model_and_mappings()\n",
    "    print(\"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\")\n",
    "\n",
    "    print(f\"ğŸ“Š ì¸í…íŠ¸ í´ë˜ìŠ¤: {len(intent2idx)}ê°œ\")\n",
    "    print(f\"ğŸ“Š ìŠ¬ë¡¯ í´ë˜ìŠ¤: {len(slot2idx)}ê°œ\")\n",
    "    print(\"\\nğŸ’¡ ì‚¬ìš©ë²•:\")\n",
    "    print(\"  - í…ìŠ¤íŠ¸ ì…ë ¥ ì‹œ ì˜ˆì¸¡ ë° ë¼ìš°íŒ… ê²°ì • ê²°ê³¼ í‘œì‹œ\")\n",
    "    print(\"  - ì„ê³„ê°’ ë³€ê²½: /threshold [ê°’]\")\n",
    "    print(\"  - ì¢…ë£Œ: exit\")\n",
    "\n",
    "\n",
    "    threshold = 0.5 # Default threshold for analyze_prediction\n",
    "    multi_threshold = 0.5 # Default threshold for make_routing_decision\n",
    "\n",
    "    while True:\n",
    "        user_input = input(f\"\\nâœ‰ï¸ ì…ë ¥ (Analyze Thresh={threshold:.2f}, Multi Thresh={multi_threshold:.2f}): \").strip()\n",
    "\n",
    "        user_input = normalize_with_morph(user_input)\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "\n",
    "        if user_input.startswith(\"/threshold\"):\n",
    "            try:\n",
    "                parts = user_input.split()\n",
    "                if len(parts) > 1:\n",
    "                    new_threshold = float(parts[1])\n",
    "                    threshold = max(0.0, min(1.0, new_threshold))\n",
    "                    print(f\"ğŸ¯ ìƒì„¸ ë¶„ì„ ì„ê³„ê°’ ë³€ê²½: {threshold:.2f}\")\n",
    "                if len(parts) > 2:\n",
    "                    new_multi_threshold = float(parts[2])\n",
    "                    multi_threshold = max(0.0, min(1.0, new_multi_threshold))\n",
    "                    print(f\"ğŸ¯ ë³µí•© ì˜ë„ ì„ê³„ê°’ ë³€ê²½: {multi_threshold:.2f}\")\n",
    "                elif len(parts) == 2:\n",
    "                    print(\"ğŸ’¡ ë³µí•© ì˜ë„ ì„ê³„ê°’ë„ í•¨ê»˜ ë³€ê²½í•˜ë ¤ë©´ `/threshold [ë¶„ì„ ì„ê³„ê°’] [ë³µí•© ì˜ë„ ì„ê³„ê°’]` í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•˜ì„¸ìš”.\")\n",
    "\n",
    "            except:\n",
    "                print(\"âŒ ì‚¬ìš©ë²•: /threshold [ë¶„ì„ ì„ê³„ê°’] [ë³µí•© ì˜ë„ ì„ê³„ê°’ (ì„ íƒ ì‚¬í•­)]\")\n",
    "            continue\n",
    "\n",
    "        # Process any input as a query\n",
    "        if user_input:\n",
    "            # Routing decision\n",
    "            routing_result = make_routing_decision(\n",
    "                user_input, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                multi_threshold=multi_threshold\n",
    "            )\n",
    "            print(f\"\\n--- ë¼ìš°íŒ… ê²°ì • ---\")\n",
    "            print(f\"ğŸ¯ ê²°ì •: {routing_result['decision'].upper()}\")\n",
    "            print(f\"ğŸ“Š ìµœëŒ€ ì‹ ë¢°ë„: {routing_result['confidence']:.4f}\")\n",
    "            print(f\"ğŸ”„ ì•¡ì…˜: {routing_result['action']}\")\n",
    "            if routing_result['intents']:\n",
    "                 intents_str = \", \".join([f\"{intent}({prob:.3f})\"\n",
    "                                          for intent, prob in routing_result['intents']])\n",
    "                 print(f\"ğŸ·ï¸ ì˜ˆì¸¡ ì˜ë„ (ì„ê³„ê°’ {multi_threshold:.2f} ì´ìƒ): {intents_str}\")\n",
    "\n",
    "            # Detailed analysis\n",
    "            print(f\"\\n--- ìƒì„¸ ì˜ˆì¸¡ ë¶„ì„ ---\")\n",
    "            analyze_prediction(\n",
    "                user_input, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "                threshold=threshold, show_all_probs=False # show_all_probsëŠ” í•­ìƒ Falseë¡œ ìœ ì§€\n",
    "            )\n",
    "\n",
    "\n",
    "# ğŸ® ê°„ë‹¨í•œ ì˜ˆì¸¡ í•¨ìˆ˜ (ê¸°ì¡´ ìŠ¤íƒ€ì¼ í˜¸í™˜)\n",
    "def predict_top_k_intents_and_slots(text, k=3, threshold=0.5):\n",
    "    \"\"\"ê¸°ì¡´ ìŠ¤íƒ€ì¼ê³¼ í˜¸í™˜ë˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì¸¡ í•¨ìˆ˜\"\"\"\n",
    "    model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot = load_model_and_mappings()\n",
    "\n",
    "    result = predict_with_bce(\n",
    "        text, model, tokenizer, intent2idx, idx2intent, slot2idx, idx2slot,\n",
    "        threshold=threshold, top_k_intents=k\n",
    "    )\n",
    "\n",
    "    # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜\n",
    "    intents = result['all_top_intents']\n",
    "    slots = result['slots']\n",
    "\n",
    "    return intents, slots\n",
    "\n",
    "\n",
    "# ğŸš€ ë©”ì¸ ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ\n",
    "    interactive_test()\n",
    "\n",
    "    # ë˜ëŠ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸\n",
    "    # intents, slots = predict_top_k_intents_and_slots(\"ë‚´ì¼ ë¹„í–‰ê¸° ì‹œê°„í‘œ ì•Œë ¤ì£¼ì„¸ìš”\")\n",
    "    # print(\"ì¸í…íŠ¸:\", intents)\n",
    "    # print(\"ìŠ¬ë¡¯:\", slots)"
   ],
   "id": "b659ef41ae74cfdf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e79bcb1a243d238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa38e95496c3e4ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16b02c3eb4734cd6"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
