import json
from typing import Dict, Any, List
from langgraph.graph import StateGraph, END
from openai import OpenAI
from functools import partial
from chatbot.graph.state import ChatState
from langchain_core.messages import HumanMessage, AIMessage
from chatbot.graph.nodes.classifiy_intent import classify_intent
import re
from chatbot.rag.llm_tools import _format_and_style_with_llm

# í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
from dotenv import load_dotenv
load_dotenv()

# ì§ì ‘ openai í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
from openai import OpenAI
client = OpenAI()

DISCLAIMER = (
    "\n\n"
    "ì£¼ì˜: ì´ ì •ë³´ëŠ” ì¸ì²œêµ­ì œê³µí•­ ì›¹ì‚¬ì´íŠ¸(ê³µì‹ ì¶œì²˜)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì œê³µë˜ì§€ë§Œ, ì‹¤ì œ ê³µí•­ ìš´ì˜ ì •ë³´ì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    "ê°€ì¥ ì •í™•í•œ ìµœì‹  ì •ë³´ëŠ” ì¸ì²œêµ­ì œê³µí•­ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ë˜ëŠ” í•´ë‹¹ í•­ê³µì‚¬/ê¸°ê´€/ì‹œì„¤ì— ì§ì ‘ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
)

# ì£¼ì˜ ë¬¸êµ¬ë¥¼ ì œê±°í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
def _remove_disclaimer(text: str) -> str:
    """í…ìŠ¤íŠ¸ì—ì„œ ë¯¸ë¦¬ ì •ì˜ëœ ì£¼ì˜ ë¬¸êµ¬ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return re.sub(re.escape(DISCLAIMER), "", text, flags=re.DOTALL).strip()

def _decompose_and_classify_queries(user_query: str, supported_intents: List[str], messages: List[Any]) -> List[Dict[str, str]]:
    """
    LLMì„ ì‚¬ìš©í•˜ì—¬ ë³µí•© ì˜ë„ ì§ˆë¬¸ì„ ë‹¨ì¼ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ê³  ì˜ë„ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    ì´ì „ ëŒ€í™” ë§¥ë½ì„ í™œìš©í•˜ì—¬ í›„ì† ì§ˆë¬¸ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    client = OpenAI() # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    supported_intents_str = ", ".join(supported_intents)

    system_prompt = f"""
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë³µí•©ì ì¸ ì§ˆë¬¸ì„ ë‹¨ì¼ ì˜ë„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
    ì „ì²´ ëŒ€í™” ê¸°ë¡ê³¼ ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì„ ë¶„í•´í•˜ê³ , ê° ë¶€ë¶„ì— ê°€ì¥ ì í•©í•œ ë‹¨ì¼ ì˜ë„ë¥¼ ì°¾ì•„ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

    ì‚¬ìš© ê°€ëŠ¥í•œ ì˜ë„ ëª©ë¡:
    {supported_intents_str}

    ì§€ì¹¨:
    1. í˜„ì¬ ì§ˆë¬¸ì´ ì´ì „ ëŒ€í™”ì˜ í›„ì† ì§ˆë¬¸ì´ë¼ë©´, ì´ì „ ë§¥ë½ì„ í¬í•¨í•˜ì—¬ ì§ˆë¬¸ì„ ì¬êµ¬ì„±í•˜ì„¸ìš”. 
       ì˜ˆì‹œ: "ì¥ê¸°ì£¼ì°¨ì¥ í˜„í™©" -> "ìš”ê¸ˆì€?" -> "ì¥ê¸°ì£¼ì°¨ì¥ ìš”ê¸ˆì€?"
    2. í•µì‹¬ ì‹œì„¤ í‚¤ì›Œë“œ ìš°ì„  ë¶„ë¥˜: 'ì•½êµ­', 'ì€í–‰', 'í™˜ì „ì†Œ', 'ì‹ë‹¹' ë“± ì‹œì„¤ì´ë‚˜ ì—…ì²´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸ì€ `facility_guide` ì˜ë„ë¡œ ìš°ì„ ì ìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
    3. ë¶„í•´ëœ ê° ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•œ ë‹¨ì¼ ì˜ë„ë¥¼ í• ë‹¹í•˜ì„¸ìš”.
    4. ì§ˆë¬¸ì„ ë¶„í•´í•  í•„ìš”ê°€ ì—†ë‹¤ë©´, ì „ì²´ ì§ˆë¬¸ê³¼ í•˜ë‚˜ì˜ ì˜ë„ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
    5. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ì€ ì¶”ê°€í•˜ì§€ ë§ê³ , ì˜¤ì§ JSON ê°ì²´ë§Œ ë°˜í™˜í•˜ì„¸ìš”.

    JSON ì‘ë‹µ í˜•ì‹:
    {{
      "decomposed_queries": [
        {{"question": "ì§ˆë¬¸ 1", "intent": "ì˜ë„ëª…1"}},
        {{"question": "ì§ˆë¬¸ 2", "intent": "ì˜ë„ëª…2"}}
      ]
    }}
    """
    
    messages_for_llm = [
        {"role": "system", "content": system_prompt}
    ]
    # ì „ì²´ ëŒ€í™” ê¸°ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
    for msg in messages:
        if isinstance(msg, HumanMessage):
            messages_for_llm.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            messages_for_llm.append({"role": "assistant", "content": msg.content})
    
    # í˜„ì¬ ì§ˆë¬¸ì„ ê°€ì¥ ë§ˆì§€ë§‰ì— ì¶”ê°€
    messages_for_llm.append({"role": "user", "content": user_query})

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=messages_for_llm,
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        result = response.choices[0].message.content
        parsed_result = json.loads(result)
        return parsed_result.get("decomposed_queries", [])
    except Exception as e:
        print(f"ë””ë²„ê·¸: LLM ì§ˆë¬¸ ë¶„í•´ ë° ì˜ë„ ë¶„ë¥˜ ì‹¤íŒ¨ - {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë˜ ì§ˆë¬¸ê³¼ ê¸°ë³¸ ì˜ë„ë¥¼ ë°˜í™˜
        return [{"question": user_query, "intent": "default"}]

def _initial_dispatch(state: ChatState, handlers: Dict[str, Any]) -> ChatState:
    """ì„œë¸Œê·¸ë˜í”„ ë‚´ì—ì„œ í•¸ë“¤ëŸ¬ë¡œ ë¼ìš°íŒ…í•˜ê¸° ìœ„í•œ ë”ë¯¸ ë…¸ë“œì…ë‹ˆë‹¤."""
    return state

def _combine_responses(original_query: str, responses: List[str]) -> str:
    """ì—¬ëŸ¬ í•¸ë“¤ëŸ¬ì˜ ì‘ë‹µì„ í•˜ë‚˜ì˜ ì‘ë‹µìœ¼ë¡œ ê²°í•©í•©ë‹ˆë‹¤."""
    if len(responses) == 1:
        return responses[0]
    
    combined_text = ""
    for response in responses:
        combined_text += f"{response.strip()}\n\n"
    
    return combined_text.strip()

# ----------------------------------------------------------------------
# ì±—ë´‡ì˜ ë©”ì¸ ê·¸ë˜í”„ì—ì„œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜
def handle_complex_intent(state: ChatState, handlers: Dict[str, Any], supported_intents: List[str]):
    """ë³µí•© ì˜ë„ ì§ˆë¬¸ì„ ë¶„ë¦¬í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜"""
    user_input = state["user_input"]
    messages = state.get("messages", [])
    print("--- ë³µí•© ì˜ë„ ì²˜ë¦¬ ì‹œì‘ ---")

    # ğŸ“Œ í•µì‹¬ ë³€ê²½ì : LLMì„ ì‚¬ìš©í•˜ì—¬ ì „ì²´ ë§¥ë½ì„ ê³ ë ¤í•œ ì§ˆë¬¸ ë¶„í•´
    decomposed_queries = _decompose_and_classify_queries(user_input, supported_intents, messages)
    print(f"ë¶„í•´ëœ ì§ˆë¬¸: {decomposed_queries}")

    all_responses = []
    
    # ë‹¨ì¼ ì˜ë„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„œë¸Œê·¸ë˜í”„ ìƒì„±
    subgraph_builder = StateGraph(ChatState)
    subgraph_builder.add_node("entry_point", partial(_initial_dispatch, handlers=handlers))
    for name, handler in handlers.items():
        subgraph_builder.add_node(name, handler)
        subgraph_builder.add_edge(name, END)
    
    subgraph_builder.set_entry_point("entry_point")
    
    def subgraph_router(state):
        intent = state.get("intent")
        return f"{intent}_handler" if f"{intent}_handler" in handlers else "default_handler"

    subgraph_builder.add_conditional_edges("entry_point", subgraph_router)
    subgraph = subgraph_builder.compile()

    for item in decomposed_queries:
        question = item["question"]
        intent = item["intent"]
        
        # ë¶„í•´ëœ ê° ì§ˆë¬¸ì— ëŒ€í•´ ìƒˆë¡œìš´ ìƒíƒœë¥¼ ë§Œë“¤ê³  ì„œë¸Œê·¸ë˜í”„ í˜¸ì¶œ
        sub_state = {"user_input": question, "intent": intent, "response": None}
        result = subgraph.invoke(sub_state)
        response_content = result.get("response", "")
        if response_content:
            cleaned_response = _remove_disclaimer(response_content)
            all_responses.append(cleaned_response)

    final_response_text = _combine_responses(user_input, all_responses)
    
    final_response = _format_and_style_with_llm(final_response_text, "complex_intent")
    final_response += DISCLAIMER
    
    print("--- ë³µí•© ì˜ë„ ì²˜ë¦¬ ì™„ë£Œ ---")
    
    state["response"] = final_response
    state["intent"] = "complex_intent"
    
    return state
