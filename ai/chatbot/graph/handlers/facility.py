from typing import List, Dict, Any
from chatbot.graph.state import ChatState
from chatbot.rag.utils import get_query_embedding, perform_vector_search, close_mongo_client
from chatbot.rag.config import RAG_SEARCH_CONFIG, common_llm_rag_caller
from chatbot.rag.llm_tools import extract_location_with_llm, _extract_facility_names_with_llm, _filter_and_rerank_docs

def _combine_individual_responses(responses: List[str]) -> str:
    """ê°œë³„ RAG í•¸ë“¤ëŸ¬ì˜ ì‘ë‹µì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    if not responses:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì‹œì„¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ì‘ë‹µì´ í•˜ë‚˜ì¼ ê²½ìš°
    if len(responses) == 1:
        return responses[0]

    # ë³µìˆ˜ì¼ ê²½ìš° ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ê²°í•©
    combined_text = "ì‚¬ìš©ìë‹˜ì˜ ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì…ë‹ˆë‹¤.\n\n"
    for idx, response in enumerate(responses, 1):
        combined_text += f"{idx}. {response}\n"
    return combined_text

def facility_guide_handler(state: ChatState) -> ChatState:
    """
    'facility_guide' ì˜ë„ì— ëŒ€í•œ RAG ê¸°ë°˜ í•¸ë“¤ëŸ¬.
    ê° ì‹œì„¤ë³„ë¡œ RAGë¥¼ ê°œë³„ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ í•©ì¹©ë‹ˆë‹¤.
    """
    query_to_process = state.get("rephrased_query") or state.get("user_input", "")
    intent_name = state.get("intent", "facility_guide")

    if not query_to_process:
        print("ë””ë²„ê·¸: ì‚¬ìš©ì ì¿¼ë¦¬ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        return {**state, "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ ë‚´ìš©ì„ íŒŒì•…í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}

    print(f"\n--- {intent_name.upper()} í•¸ë“¤ëŸ¬ ì‹¤í–‰ ---")
    print(f"ë””ë²„ê·¸: í•¸ë“¤ëŸ¬ê°€ ì²˜ë¦¬í•  ìµœì¢… ì¿¼ë¦¬ - '{query_to_process}'")

    # 1. LLMì„ ì‚¬ìš©í•˜ì—¬ ìœ„ì¹˜ ì •ë³´ì™€ ì‹œì„¤ ì´ë¦„ ëª©ë¡ ì¶”ì¶œ
    location_keyword = extract_location_with_llm(query_to_process)
    facility_names = _extract_facility_names_with_llm(query_to_process)
    print(f"ë””ë²„ê·¸: LLMìœ¼ë¡œ ì¶”ì¶œëœ ìœ„ì¹˜ ì •ë³´ - {location_keyword}")
    print(f"ë””ë²„ê·¸: LLMì„ ì‚¬ìš©í•´ ì¶”ì¶œëœ ì‹œì„¤ ëª©ë¡ - {facility_names}")

    if not facility_names:
        return {**state, "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì‹œì„¤ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
    rag_config = RAG_SEARCH_CONFIG.get(intent_name, {})
    collection_name = rag_config.get("collection_name")
    vector_index_name = rag_config.get("vector_index_name")
    intent_description = rag_config.get("description", intent_name)
    query_filter = rag_config.get("query_filter")

    if not (collection_name and vector_index_name):
        error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. '{intent_name}' ì˜ë„ì— ëŒ€í•œ ì •ë³´ ê²€ìƒ‰ ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ì¸ë±ìŠ¤ ì´ë¦„ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤."
        print(f"ë””ë²„ê·¸: {error_msg}")
        return {**state, "response": error_msg}

    individual_responses = []
    try:
        # ğŸ“Œ ìˆ˜ì •ëœ ë¡œì§: ê° ì‹œì„¤ ì´ë¦„ë³„ë¡œ ì™„ì „í•œ RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰
        for facility_name in facility_names:
            print(f"ë””ë²„ê·¸: '{facility_name}'ì— ëŒ€í•œ RAG íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
            
            # 1ë‹¨ê³„: ë„“ì€ ë²¡í„° ê²€ìƒ‰
            query_embedding = get_query_embedding(facility_name)
            retrieved_docs_text = perform_vector_search(
                query_embedding,
                collection_name=collection_name,
                vector_index_name=vector_index_name,
                query_filter=query_filter,
                top_k=10
            )
            print(f"ë””ë²„ê·¸: '{facility_name}'ì— ëŒ€í•´ {len(retrieved_docs_text)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")

            # 2ë‹¨ê³„: í•„í„°ë§ ë° ì¬ì •ë ¬
            context_for_llm = "\n\n".join(retrieved_docs_text)
            final_context = context_for_llm # í•„í„°ë§ì„ ê±´ë„ˆë›°ê³  ëª¨ë“  ë¬¸ì„œë¥¼ ì‚¬ìš©

            if not final_context and retrieved_docs_text:
                print(f"ë””ë²„ê·¸: '{facility_name}' í•„í„°ë§ ì‹¤íŒ¨. ì›ë³¸ ë¬¸ì„œë¡œ ë‹µë³€ ìƒì„± ì‹œë„.")
                final_context = context_for_llm
            
            # 3ë‹¨ê³„: ìµœì¢… LLM ë‹µë³€ ìƒì„±
            if final_context:
                truncated_docs_list = final_context.split('\n\n')[:10]
                final_context_truncated = "\n\n".join(truncated_docs_list)
                
                sub_query_to_process = f"'{location_keyword}'ì— ìˆëŠ” '{facility_name}'ì— ëŒ€í•œ ì •ë³´"
                
                final_response_text = common_llm_rag_caller(
                    sub_query_to_process,
                    final_context_truncated,
                    intent_description,
                    intent_name
                )
                individual_responses.append(final_response_text)
            else:
                individual_responses.append(f"ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  '{location_keyword}'ì— ìˆëŠ” '{facility_name}' ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        error_msg = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        print(f"ë””ë²„ê·¸: {error_msg}")
        return {**state, "response": error_msg}
    finally:
        close_mongo_client()

    # ğŸ“Œ ìˆ˜ì •ëœ ë¡œì§: ëª¨ë“  ê°œë³„ ì‘ë‹µì„ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    final_response = _combine_individual_responses(individual_responses)
    return {**state, "response": final_response}