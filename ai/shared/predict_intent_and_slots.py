import torch
from torch.nn.functional import softmax, sigmoid

from shared.load_model import model, idx2intent, idx2slot, intent2idx
from shared.normalize_with_morph import normalize_with_morph
from shared.utils import tokenizer, device

# ðŸ§± í† í° â†’ ë‹¨ì–´ ë³‘í•© + ìŠ¬ë¡¯ ì •ë ¬
def merge_tokens_and_slots(tokens, slot_ids, idx2slot):
    merged = []
    word = ''
    current_slot = ''

    for token, slot_id in zip(tokens, slot_ids):
        slot = idx2slot.get(slot_id, 'O')

        if token in ['[CLS]', '[SEP]', '[PAD]']:
            continue

        if token.startswith("â–"):  # ìƒˆ ë‹¨ì–´ ì‹œìž‘
            if word:
                merged.append((word, current_slot))
            word = token[1:]
            current_slot = slot
        else:
            word += token.replace("â–", "")

    if word:
        merged.append((word, current_slot))

    return merged

# ðŸ”® ì˜ˆì¸¡ í•¨ìˆ˜
def predict_top_k_intents_and_slots(text, k=3):
    encoding = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=64
    )
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        intent_logits, slot_logits = model(input_ids, attention_mask)
        intent_probs = softmax(intent_logits, dim=1)

        # í† í° ë””ë²„ê¹…ìš©
        # for i, logit in enumerate(slot_logits[0]):
        #   top_id = logit.argmax().item()
        #   top_slot = idx2slot[top_id]
        #   print(f"{i}ë²ˆì§¸ í† í° â†’ {top_slot} ({logit[top_id].item():.4f})")

        # ðŸŽ¯ ì¸í…íŠ¸ ì˜ˆì¸¡ (Top-k)
        topk_probs, topk_indices = torch.topk(intent_probs, k, dim=1)
        intents = [(idx2intent[idx.item()], prob.item()) for idx, prob in zip(topk_indices[0], topk_probs[0])]

        # ðŸŽ¯ ìŠ¬ë¡¯ ì˜ˆì¸¡
        slot_pred_ids = torch.argmax(slot_logits, dim=2)[0].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])

        merged = merge_tokens_and_slots(tokens, slot_pred_ids, idx2slot)

    return intents, merged

# ðŸ”® ì˜ˆì¸¡ í•¨ìˆ˜ (ì˜ë„ top-1ë§Œ ì‚¬ìš©)
def predict_intent(text):
    encoding = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=64)
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        intent_logits, _ = model(input_ids, attention_mask)
        intent_probs = softmax(intent_logits, dim=1)
        top_prob, top_index = torch.max(intent_probs, dim=1)
        predicted_intent = idx2intent[top_index.item()]
        return predicted_intent, top_prob.item()

# ðŸ”® BCEWithLogitsLoss ê¸°ë°˜ ì˜ˆì¸¡ í•¨ìˆ˜
def predict_with_bce(text, threshold=0.8, top_k_intents=3, max_length=64):
    """
    BCEWithLogitsLossë¡œ í•™ìŠµëœ ëª¨ë¸ì„ ìœ„í•œ ì˜ˆì¸¡ í•¨ìˆ˜

    Args:
        text: ìž…ë ¥ í…ìŠ¤íŠ¸
        threshold: Intent ë¶„ë¥˜ ìž„ê³„ê°’ (default: 0.8)
        top_k_intents: ìƒìœ„ Kê°œ ì¸í…íŠ¸ ë°˜í™˜ (default: 3)
    """
    text = normalize_with_morph(text)
    encoding = tokenizer(
        text,
        return_tensors='pt',
        truncation=True,
        padding='max_length',
        max_length=max_length
    )
    input_ids = encoding["input_ids"].to(device)
    attention_mask = encoding["attention_mask"].to(device)

    with torch.no_grad():
        intent_logits, slot_logits = model(input_ids, attention_mask)

        # Intent ì˜ˆì¸¡ (Sigmoid ê¸°ë°˜)
        intent_probs = sigmoid(intent_logits)[0]  # [num_intents]

        # ìž„ê³„ê°’ ì´ìƒì˜ ì¸í…íŠ¸ë“¤ ì°¾ê¸°
        high_confidence_intents = []
        for i, prob in enumerate(intent_probs):
            if prob.item() >= threshold:
                intent_name = idx2intent[i]
                high_confidence_intents.append((intent_name, prob.item()))

        # í™•ë¥  ìˆœìœ¼ë¡œ ì •ë ¬
        high_confidence_intents.sort(key=lambda x: x[1], reverse=True)

        # ë§Œì•½ ìž„ê³„ê°’ ì´ìƒì¸ ê²Œ ì—†ë‹¤ë©´ ìµœê³  í™•ë¥  í•˜ë‚˜ë§Œ
        if not high_confidence_intents:
            max_idx = torch.argmax(intent_probs).item()
            max_prob = intent_probs[max_idx].item()
            high_confidence_intents = [(idx2intent[max_idx], max_prob)]

        # Top-K ì¸í…íŠ¸ (ì „ì²´ ìˆœìœ„ìš©)
        topk_probs, topk_indices = torch.topk(intent_probs, min(top_k_intents, len(intent2idx)))
        all_top_intents = [(idx2intent[idx.item()], prob.item())
                          for idx, prob in zip(topk_indices, topk_probs)]

        # ìŠ¬ë¡¯ ì˜ˆì¸¡ (ê¸°ì¡´ê³¼ ë™ì¼ - Softmax ê¸°ë°˜)
        slot_pred_ids = torch.argmax(slot_logits, dim=2)[0].tolist()
        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])
        merged_slots = merge_tokens_and_slots(tokens, slot_pred_ids, idx2slot)

    return {
        'high_confidence_intents': high_confidence_intents,  # ìž„ê³„ê°’ ì´ìƒ
        'all_top_intents': all_top_intents,                  # ì „ì²´ Top-K
        'slots': merged_slots,
        'is_multi_intent': len(high_confidence_intents) > 1,
        'max_intent_prob': max(prob for _, prob in all_top_intents),
        'intent_probs_raw': intent_probs.cpu().numpy()
    }