import torch
import torch.nn as nn
from torch.nn.functional import softmax
from transformers import BertModel, AutoTokenizer
import pickle


# ğŸ“Œ KoBERTIntentSlotModel ë¡œë“œ
class KoBERTIntentSlotModel(nn.Module):
    def __init__(self, num_intents, num_slots):
        super().__init__()
        self.bert = BertModel.from_pretrained("skt/kobert-base-v1")
        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, num_intents)
        self.slot_classifier = nn.Linear(self.bert.config.hidden_size, num_slots)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        pooled_output = outputs.pooler_output

        intent_logits = self.intent_classifier(pooled_output)
        slot_logits = self.slot_classifier(sequence_output)

        return intent_logits, slot_logits


# ğŸ”§ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… í† í¬ë‚˜ì´ì € ë° ì¸í…íŠ¸ ì¸ë±ìŠ¤ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("skt/kobert-base-v1", use_fast=False)
with open("best_models/intent-slot-kobert/intent2idx.pkl", "rb") as f:
    intent2idx = pickle.load(f)
idx2intent = {v: k for k, v in intent2idx.items()}

# âœ… ëª¨ë¸ ë¡œë“œ
model = KoBERTIntentSlotModel(num_intents=len(intent2idx), num_slots=19)
model.load_state_dict(torch.load("best_models/intent-slot-kobert/best_model.pt", map_location=device))
model.to(device)
model.eval()


# ğŸ”® ì˜ˆì¸¡ í•¨ìˆ˜
def predict_top_k_intents_and_slots(text, k=3):
    encoding = tokenizer(
        text,
        truncation=True,
        padding='max_length',
        max_length=64,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        intent_logits, slot_logits = model(input_ids, attention_mask)
        intent_probs = softmax(intent_logits, dim=1)

        # ğŸ¯ ì¸í…íŠ¸ Top-K
        topk_probs, topk_indices = torch.topk(intent_probs, k, dim=1)
        intents = [(idx2intent[topk_indices[0][i].item()], topk_probs[0][i].item()) for i in range(k)]

        # ğŸ¯ ìŠ¬ë¡¯ ì˜ˆì¸¡
        slot_pred = torch.argmax(slot_logits, dim=2)[0].tolist()
        tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])

        # ìŠ¬ë¡¯ ì¸ë±ìŠ¤ ë¡œë“œ
        with open("best_models/intent-slot-kobert/slot2idx.pkl", "rb") as f:
            slot2idx = pickle.load(f)
        idx2slot = {v: k for k, v in slot2idx.items()}

        # ğŸ·ï¸ ì‹¤ì œ í…ìŠ¤íŠ¸ ë‹¨ì–´ ë‹¨ìœ„ì— ëŒ€ì‘í•˜ëŠ” í† í° + ìŠ¬ë¡¯ë§Œ ì¶”ì¶œ (special token ì œì™¸)
        words_with_slots = []
        for token, slot_id in zip(tokens, slot_pred):
            if token not in ["[CLS]", "[SEP]", "[PAD]"]:
                words_with_slots.append((token, idx2slot[slot_id]))

    return intents, words_with_slots


if __name__ == "__main__":
    while True:
        text = input("âœ‰ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
        if text.lower() == 'exit':
            break
        intents, word_slots = predict_top_k_intents_and_slots(text, k=3)

        print("ğŸ” ì˜ˆì¸¡ëœ ì¸í…íŠ¸ TOP 3:")
        for intent, conf in intents:
            print(f" - {intent}: {conf:.4f}")

        print("ğŸ¯ ì˜ˆì¸¡ëœ ìŠ¬ë¡¯ ì •ë³´:")
        for word, slot in word_slots:
            print(f" - {word}: {slot}")